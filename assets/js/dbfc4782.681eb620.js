"use strict";(self.webpackChunkfreewili_docs=self.webpackChunkfreewili_docs||[]).push([[8749],{1895:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"block-cycle","metadata":{"permalink":"/blog/block-cycle","source":"@site/blog/2025-10-17/block-cycle.mdx","title":"Block Cycle","description":"Have you ever wondered if your product is recyclable? Look no further; our program analyzes if a product is recyclable or not using our camera technology and provides steps on how to recycle it.","date":"2025-10-17T00:00:00.000Z","tags":[{"inline":false,"label":"FREE-WILi","permalink":"/blog/tags/FREE-WILi","description":"Make Embedded Systems Fun Again !"},{"inline":false,"label":"Hack Dearborn","permalink":"/blog/tags/hackdearborn","description":"hackdearborn"},{"inline":false,"label":"Student Project","permalink":"/blog/tags/studentproject","description":"studentproject"},{"inline":false,"label":"Hackathon","permalink":"/blog/tags/hackathon","description":"hackathon"}],"readingTime":2.72,"hasTruncateMarker":true,"authors":[{"name":"Natalie K","url":"https://devpost.com/nbkim481","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Natalie K","page":null},{"name":"Ty Senopole","url":"https://devpost.com/tsenopole","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Ty Senopole","page":null},{"name":"Nimbus72727","url":"https://devpost.com/Nimbus72727","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Nimbus72727","page":null},{"name":"Suho-Park1 Park","url":"https://devpost.com/Suho-Park1","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Suho-Park1 Park","page":null}],"frontMatter":{"slug":"block-cycle","title":"Block Cycle","authors":["Natalie K","Ty Senopole","Nimbus72727","Suho-Park1 Park"],"date":"2025-10-17T00:00:00.000Z","tags":["FREE-WILi","Hack Dearborn","student project","hackathon"]},"unlisted":false,"nextItem":{"title":"DriveGuard","permalink":"/blog/driveguard"}},"content":"> Have you ever wondered if your product is recyclable? Look no further; our program analyzes if a product is recyclable or not using our camera technology and provides steps on how to recycle it.\\r\\n\\r\\nimport MyCarousel from \'@site/src/components/Carousel\';\\r\\n\\r\\n{/* <MyCarousel /> */}\\r\\n\\r\\nexport const slides = [\\r\\n  { type: \'video\', src: \'5N4ZulY_rLs\' },\\r\\n];\\r\\n\\r\\n<MyCarousel slides={slides} />\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n## Inspiration\\r\\n\\r\\nThe original inspiration was to scan a product barcode and receive recycling information. After being introduced to the Free-Wili, a microcontroller with a built-in screen, we switched gears to gamifying recycling with a virtual pet, as well as using image detection.\\r\\n\\r\\n## What it does\\r\\n\\r\\nUsing the Free-Wili, we use the camera module to capture a photo of a product that the user is ready to dispose. From there, a Python program sends the photo to the Gemini API. It identifies what is found in the image, if it is recyclable, and provides steps on how to prep the item for recycling. Then, a Pygame program reads the output from the Gemini API and outputs an image of our virtual pet in either a happy state or a sad state, depending on whether the product is recyclable or not. Additionally, there is a health system where the pet receives or loses ten points depending on what product is scanned.\\r\\n\\r\\n## How we built it\\r\\n\\r\\nWe used the Free-Wili and the WILLEYE Camera Orca attachment to take a picture. We also used Python and Gemini for image processing. Pygame was used to handle the visual elements for the virtual pet.\\r\\n\\r\\n## Challenges we ran into\\r\\n\\r\\nLearning how to use the Free-Wili from scratch because it not only new hardware but also new software for everyone in the team. We ran into issues with image detection while setting up Gemini. Unfortunately, some ideas had to be scrapped. We wanted to program a button on the Free-Wili so that it takes and saves the picture on the computer. We also wanted the virtual pet to be displayed on the Free-Wili device; however, we were unable to transfer the images from the computer to the display.\\r\\n\\r\\n## Accomplishments that we\'re proud of\\r\\n\\r\\nWe were able to get the WILLEYE camera to capture images. The image processing and prompt engineering gave a consistent and viable response. We were able to find ways around our ideas with the limited knowledge we had of the hardware within 24 hours.\\r\\n\\r\\n## What we learned\\r\\n\\r\\nWe learned how to use the Free-Wili and what it is capable of. We learned how to implement an API for image processing. We learned how to use GitHub to better collaborate on a larger-scale project.\\r\\n\\r\\n## What\'s next for Block Cycle\\r\\n\\r\\nIf we had more time, we would figure out how to display the virtual pet on the screen on the Free-Wili. We would also make the specific analysis of the images be played through text-to-speech to the user. We would also provide where the items could be recycled in the response.\\r\\n\\r\\n## Built With\\r\\n\\r\\n`free-wili`\\r\\n`gemini`\\r\\n`genai`\\r\\n`python`\\r\\n`wileye-camera-orca`\\r\\n\\r\\n### Try it out\\r\\n\\r\\n<div class=\\"github-img\\">\\r\\n\\r\\n![GitHub](./github.png) <a href=\\"https://github.com/Ty-Cheng5/HackDearborn4\\" target=\\"_blank\\"><span>GitHub Repo</span></a>\\r\\n</div>\\r\\n\\r\\n*Source* - [https://devpost.com/software/beatbox-jrpklt](https://devpost.com/software/beatbox-jrpklt)"},{"id":"driveguard","metadata":{"permalink":"/blog/driveguard","source":"@site/blog/2025-10-17/driveguard.mdx","title":"DriveGuard","description":"DriveGuard, powered by FREE-WILi, spots distracted drivers in real time and lights up to keep you focused on the road.","date":"2025-10-17T00:00:00.000Z","tags":[{"inline":false,"label":"FREE-WILi","permalink":"/blog/tags/FREE-WILi","description":"Make Embedded Systems Fun Again !"},{"inline":false,"label":"Hack Dearborn","permalink":"/blog/tags/hackdearborn","description":"hackdearborn"},{"inline":false,"label":"Student Project","permalink":"/blog/tags/studentproject","description":"studentproject"},{"inline":false,"label":"Hackathon","permalink":"/blog/tags/hackathon","description":"hackathon"}],"readingTime":2.01,"hasTruncateMarker":true,"authors":[{"name":"Ameer Kayed","url":"https://devpost.com/ameerkayed34","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Ameer Kayed","page":null}],"frontMatter":{"slug":"driveguard","title":"DriveGuard","authors":["Ameer Kayed"],"date":"2025-10-17T00:00:00.000Z","tags":["FREE-WILi","Hack Dearborn","student project","hackathon"]},"unlisted":false,"prevItem":{"title":"Block Cycle","permalink":"/blog/block-cycle"},"nextItem":{"title":"4kHD_Accessibility","permalink":"/blog/4khd_accessibility"}},"content":"> DriveGuard, powered by FREE-WILi, spots distracted drivers in real time and lights up to keep you focused on the road.\\r\\n\\r\\nimport MyCarousel from \'@site/src/components/Carousel\';\\r\\n\\r\\n{/* <MyCarousel /> */}\\r\\n\\r\\nexport const slides = [\\r\\n  { type: \'video\', src: \'rlxwuNzEJOg\' },\\r\\n];\\r\\n\\r\\n<MyCarousel slides={slides} />\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n## Inspiration\\r\\n\\r\\nDistracted driving causes so many preventable accidents every year. I wanted to create something that reacts in real time to help drivers stay focused. That idea became **DriveGuard**, powered by FREE-WILi, a system that monitors where the driver is looking and uses LED feedback to keep them alert.\\r\\n\\r\\n## What it does\\r\\n\\r\\n- **DriveGuard** uses computer vision to track the driver\u2019s eyes and head position.\\r\\n- If the driver is distracted, the **FREE-WILi LEDs turn red** to warn them.\\r\\n- If the driver is focused, the LEDs stay **green**.\\r\\n- Everything happens live, giving the driver constant awareness of their focus on the road.\\r\\n\\r\\n## How we built it\\r\\n\\r\\nThe system was built using **Python**, **OpenCV**, and **MediaPipe** for tracking, along with **FREE-WILi scripts** to control the LED indicators. A simulated driving video runs beside the webcam feed to mimic a real driving environment and test the system\u2019s accuracy.\\r\\n\\r\\n## Challenges we ran into\\r\\n\\r\\nIt was challenging to work with a system I had no prior experience in. The **FREE-WILi scripts** were difficult to find and understand at first, so I had to do a lot of research and testing to get everything working properly. Integrating new hardware with computer vision took time and patience.\\r\\n\\r\\n## Accomplishments that we\'re proud of\\r\\n\\r\\n- Fully functional system\\r\\n- Successful integration with **FREE-WILi**\\r\\n- LED light feedback that clearly shows driver status\\r\\n- Real-time system that runs smoothly\\r\\n\\r\\n## What we learned\\r\\n\\r\\nI learned how hard it can be to bring multiple systems together, but also how rewarding it feels when it works. I also discovered how powerful **computer vision** is \u2014 especially **MediaPipe**. It was amazing to see it detect distraction even when my head was straight but my eyes were wandering.\\r\\n\\r\\n## What\'s next for DriveGuard\\r\\nNext, I want to integrate the **FREE-WILi bracelets** for more feedback and possibly add a **beep sound** when distraction is detected. I also plan to **optimize the system** to reduce delay and make it react even faster.\\r\\n\\r\\n## Built With\\r\\n\\r\\n`mediapipe`\\r\\n`opencv`\\r\\n`python`\\r\\n\\r\\n### Try it out\\r\\n\\r\\n<div class=\\"github-img\\">\\r\\n\\r\\n![GitHub](./github.png) <a href=\\"https://github.com/ameerk34/DriveGuard\\" target=\\"_blank\\"><span>GitHub Repo</span></a>\\r\\n</div>\\r\\n\\r\\n*Source* - [https://devpost.com/software/driveguard](https://devpost.com/software/driveguard)"},{"id":"4khd_accessibility","metadata":{"permalink":"/blog/4khd_accessibility","source":"@site/blog/2025-10-17/index.mdx","title":"4kHD_Accessibility","description":"Motion & voice control for computers. We map wrist movements to keystrokes and use voice for web scrolling, giving users hands-free input control.","date":"2025-10-17T00:00:00.000Z","tags":[{"inline":false,"label":"FREE-WILi","permalink":"/blog/tags/FREE-WILi","description":"Make Embedded Systems Fun Again !"},{"inline":false,"label":"Hack Dearborn","permalink":"/blog/tags/hackdearborn","description":"hackdearborn"},{"inline":false,"label":"Student Project","permalink":"/blog/tags/studentproject","description":"studentproject"},{"inline":false,"label":"Hackathon","permalink":"/blog/tags/hackathon","description":"hackathon"}],"readingTime":3.42,"hasTruncateMarker":true,"authors":[{"name":"Fazal Rahaman Pasha Mohammed","url":"https://devpost.com/fazalfreestyle","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Fazal Rahaman Pasha Mohammed","page":null},{"name":"Derek Servin","url":"https://devpost.com/Derek-Servin","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Derek Servin","page":null},{"name":"Heisenberg217","url":"https://devpost.com/Heisenberg217","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Heisenberg217","page":null},{"name":"Jaivanth Melanaturu","url":"https://devpost.com/jaivanth779","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Jaivanth Melanaturu","page":null}],"frontMatter":{"slug":"4khd_accessibility","title":"4kHD_Accessibility","authors":["Fazal Rahaman Pasha Mohammed","Derek Servin","Heisenberg217","Jaivanth Melanaturu"],"date":"2025-10-17T00:00:00.000Z","tags":["FREE-WILi","Hack Dearborn","student project","hackathon"]},"unlisted":false,"prevItem":{"title":"DriveGuard","permalink":"/blog/driveguard"},"nextItem":{"title":"Willi: Your Care Companion","permalink":"/blog/willi-your-care-companion"}},"content":"> Motion & voice control for computers. We map wrist movements to keystrokes and use voice for web scrolling, giving users hands-free input control.\\r\\n\\r\\n## 4kHD_Accessibility: Alternative Input Control System\\r\\n\\r\\n## \ud83d\udca1 Inspiration\\r\\n\\r\\nThe primary goal of **4kHD_Accessibility** is to address the significant barrier that traditional keyboards and mice present to individuals with various disabilities, particularly those with limited upper body mobility or dexterity. We were inspired to create an **intuitive and feature-rich** alternative input method that offers comprehensive control over computing interfaces using accessible physical movements and voice commands.\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n## \ud83d\udcbb What it does\\r\\n\\r\\nOur project provides a versatile alternative input control system based on physical motion and voice commands.\\r\\n\\r\\n**1. Motion-to-Keystroke Mapping (Wrist Control):**\\r\\n\\r\\n- Utilizes motion-tracking to capture simple, distinct wrist movements.\\r\\n- Maps these movements directly to standard keyboard **keystrokes** (e.g., WASD, arrow keys, Space).\\r\\n- This allows for functional control over games and applications, as demonstrated in our included demos.\\r\\n\\r\\n**2. Voice-Activated Web Navigation:**\\r\\n\\r\\n- Enables hands-free web interaction.\\r\\n- Users can issue voice commands to **scroll up and scroll down** on any web page.\\r\\n\\r\\n\ud83d\udee0\ufe0f How we built it\\r\\n\\r\\nWe built **4kHD_Accessibility** by combining specialized hardware integration with robust software models.\\r\\n\\r\\n- **Motion Control (Wrist):**\\r\\n  - We utilized the **FREE-WILi\'s** platform to capture motion data.\\r\\n  - We developed custom Python scripts ( see `scripts/one_wili_wasd.py`, `combo_wili.py` ) to process the sensor data and translate distinct wrist movements into clean keystroke outputs.\\r\\n\\r\\n- **Voice Integration:**\\r\\n  - We incorporated **Google\'s Speech-to-Text model** via the **Python SpeechRecognition library**.\\r\\n  - This converts spoken commands into text, which our scripts ( e.g., `scripts/aud.py` , `scripts/aud_with_search.py` ) then map to web scroll actions.\\r\\n\\r\\n- **Demonstrations:**\\r\\n  - The project includes several demo games (**Demo/**) to immediately showcase the real-world usability of the motion control input.\\r\\n\\r\\n## \ud83d\udcc1 Repository Structure\\r\\n\\r\\nThe project is organized into two main folders:\\r\\n\\r\\n| Folder   | Contents                                                                                                                                            | Description                                                                                                                         |\\r\\n|----------|-----------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|\\r\\n| Demo/    | `Chess.html`, `chess.py`, `connect4.html`, `connect4.py`, `tron.py`                                                                                 | Contains files for simple web-based and application-based games to demonstrate motion-to-keystroke control in a real-world setting. |\\r\\n| scripts/ | `aud.py`, `aud_with_search.py`, `audiocapture.py`, `both_wili.py`, `combo_wili.py`, `combo_with_audio.py`, `live_cam.py`, `one_wili_wasd.py`        | The core Python scripts that handle motion data capture, voice recognition, keystroke mapping, and camera/system interaction.       |\\r\\n\\r\\n## \ud83d\uded1 Challenges we ran into\\r\\n\\r\\n- **Defining Actionable Control**: Creating the different motions for actionable control was challenging. We had to find a balance between motions that are simple enough for users to perform consistently, yet distinct enough to be reliably recognized by the system without false positives.\\r\\n- **FreeWilis Limitations**: The **FreeWilis Camera doesn\'t have native live feed support**. This required us to develop technical workarounds to process the motion data effectively in near real-time, which was critical for maintaining responsiveness.\\r\\n\\r\\n## \ud83c\udfc6 Accomplishments that we\'re proud of\\r\\n\\r\\nDespite the challenges, we successfully delivered a robust proof-of-concept:\\r\\n\\r\\n- **Functional Motion Controller**: We successfully built a controller that can identify different motions and map those motions to keystrokes for **real-world examples**, as demonstrated by the working games in the `Demo/` folder.\\r\\n- **Focused Voice Control**: We were able to implement precise voice control to handle **page scrolling**, which is a key interaction for hands-free web browsing.\\r\\n\\r\\n## \ud83e\udde0 What we learned\\r\\n\\r\\nThis project provided deep insights into accessibility development:\\r\\n\\r\\n- We gained a thorough understanding of the **FreeWili** platform and its technical documentation.\\r\\n- We learned how to integrate and optimize **Google\'s Speech-to-Text model** for application control.\\r\\n- We mastered the essentials of developing robust algorithms for accessible **motion control**.\\r\\n\\r\\n## \u23ed\ufe0f What\'s next for 4kHD_Accessibility\\r\\n\\r\\nThis is just the beginning. Our vision for the project\'s future includes:\\r\\n\\r\\n- **Real-World Deployment**: Potential for better implementation for **real-world deployment**. This includes creating a stable, installable application and conducting extensive user testing with the target community.\\r\\n- **Expanded Voice Commands**: Expanding the voice command set beyond scrolling to allow interaction with links, buttons, and form inputs.\\r\\n- **Advanced Gesture Library**: Developing a wider range of recognized motion and gesture commands for more complex, fine-grained control.\\r\\n\\r\\n#### Developed at HackDearborn 4 by Jaivanth, Paul, Fazal, Derek.\\r\\n\\r\\n## Built With\\r\\n\\r\\n`chatgpt`\\r\\n`gemini`\\r\\n`python`\\r\\n\\r\\n### Try it out\\r\\n\\r\\n<div class=\\"github-img\\">\\r\\n\\r\\n![GitHub](./github.png) <a href=\\"https://github.com/Derek-Servin/4kHD_30fps\\" target=\\"_blank\\"><span>GitHub Repo</span></a>\\r\\n</div>\\r\\n\\r\\n*Source* - [https://devpost.com/software/4khd_accessibility](https://devpost.com/software/4khd_accessibility)"},{"id":"willi-your-care-companion","metadata":{"permalink":"/blog/willi-your-care-companion","source":"@site/blog/2025-10-17/willi-your-care-companion.mdx","title":"Willi: Your Care Companion","description":"Willi: a friendly companion who listens, speaks your language, and helps when needed. One press for comfort, answers, or SOS so you feel safe and never alone.","date":"2025-10-17T00:00:00.000Z","tags":[{"inline":false,"label":"FREE-WILi","permalink":"/blog/tags/FREE-WILi","description":"Make Embedded Systems Fun Again !"},{"inline":false,"label":"Hack Dearborn","permalink":"/blog/tags/hackdearborn","description":"hackdearborn"},{"inline":false,"label":"Student Project","permalink":"/blog/tags/studentproject","description":"studentproject"},{"inline":false,"label":"Hackathon","permalink":"/blog/tags/hackathon","description":"hackathon"}],"readingTime":3.24,"hasTruncateMarker":true,"authors":[{"name":"Vikram Velankar","url":"https://devpost.com/vikramdv","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Vikram Velankar","page":null},{"name":"Shrenik Jadhav","url":"https://devpost.com/shrenikj","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Shrenik Jadhav","page":null},{"name":"Birva Sevak","url":"https://devpost.com/birvas","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Birva Sevak","page":null},{"name":"Sai Arunanshu Govindarajula","url":"https://devpost.com/saiarun","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Sai Arunanshu Govindarajula","page":null}],"frontMatter":{"slug":"willi-your-care-companion","title":"Willi: Your Care Companion","authors":["Vikram Velankar","Shrenik Jadhav","Birva Sevak","Sai Arunanshu Govindarajula"],"date":"2025-10-17T00:00:00.000Z","tags":["FREE-WILi","Hack Dearborn","student project","hackathon"]},"unlisted":false,"prevItem":{"title":"4kHD_Accessibility","permalink":"/blog/4khd_accessibility"},"nextItem":{"title":"Agent Unblind","permalink":"/blog/agent-unblind"}},"content":"> Willi: a friendly companion who listens, speaks your language, and helps when needed. One press for comfort, answers, or SOS so you feel safe and never alone.\\r\\n\\r\\nimport MyCarousel from \'@site/src/components/Carousel\';\\r\\n\\r\\n{/* <MyCarousel /> */}\\r\\n\\r\\nexport const slides = [\\r\\n  { type: \'video\', src: \'HoqtQCyNO5o\' },\\r\\n  { type: \'image\', src: \'/img/blog/willi-your-care-companion.jpg\', alt: \'willi-your-care-companion\', caption: \\"Title and Team Introduction\\" },\\r\\n  { type: \'image\', src: \'/img/blog/willi-your-care-companion-2.jpg\', alt: \'willi-your-care-companion\', caption: \\"The Global Challenge\\" },\\r\\n  { type: \'image\', src: \'/img/blog/willi-your-care-companion-3.jpg\', alt: \'willi-your-care-companion\', caption: \\"Introducing Care Companion, Willi: Your Care Companion\\" },\\r\\n  { type: \'image\', src: \'/img/blog/willi-your-care-companion-4.jpg\', alt: \'willi-your-care-companion\', caption: \\"The Magical Five Buttons\\" },\\r\\n  { type: \'image\', src: \'/img/blog/willi-your-care-companion-5.jpg\', alt: \'willi-your-care-companion\', caption: \\"Conclusion\\" },\\r\\n];\\r\\n\\r\\n<MyCarousel slides={slides} />\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n<br/>\\r\\n\\r\\n## About the Project \u2014 Willi: Your Care Companion\\r\\n\\r\\nWe didn\u2019t start with a product. We started with people\u2014our grandparents, a neighbor who lives alone, an aunt who\u2019s nervous at the doctor because English isn\u2019t her first language. We kept hearing the same quiet problems: \u201cI don\u2019t want to bother anyone,\u201d \u201cI didn\u2019t understand what they said,\u201d \u201cI panicked and forgot what to do.\u201d That\u2019s where Willi began\u2014not as tech, but as a promise to make tough moments feel smaller and less lonely.\\r\\n\\r\\nAt first we sketched way too much: apps, dashboards, ten different modes. Then we watched our test users fumble with menus and tiny icons and we realized: in a tough moment you only want one thing press, and something helpful happens. That\u2019s how we landed on five simple buttons. No hunting, no swiping, just muscle memory.\\r\\n\\r\\nWe built the first prototype on a FreeWili/ESP-style board with big, tactile buttons. Red became SOS. Blue was \u201ctalk to me\u201d a friendly companion that listens and turns worries into clear next steps. Yellow handled translation so anyone could speak and the user would hear it in their own language. White sent discreet beeps to nearby devices when saying \u201chelp\u201d out loud wasn\u2019t safe. Green let you snap a photo and hear what it is, surprisingly powerful for low-vision moments like \u201cIs this the right medicine?\u201d\\r\\n\\r\\nThe wiring was the easy part. The hard parts were human: Simplicity vs. power. We kept asking, \u201cWould my grandma understand this at 2 a.m.?\u201d If the answer was \u201cmaybe,\u201d we cut it.\\r\\n\\r\\nTrustworthy guidance. Health advice had to sound calm and plain, not robotic. We iterated a lot on tone and step-by-step phrasing.\\r\\n\\r\\nLanguage is personal. Translation isn\u2019t just words; it\u2019s respect. We tuned it to feel natural and kind.\\r\\n\\r\\nOffline first. We didn\u2019t want the device to panic just because the Wi-Fi did. Local alerts and simple fallbacks became non-negotiables.\\r\\n\\r\\nWe learned that \u201cfriendliness\u201d isn\u2019t a feature you toggle it\u2019s the sum of a thousand tiny choices: the button feel, the way the voice greets you, how instructions pause and wait for you to breathe. We learned that one clear step beats ten clever ones. And we learned that people open up when they feel seen and unhurried.\\r\\n\\r\\nIs Willi finished? Not even close. Next, we want pilots in senior centers and clinics, more languages that sound like home, and a gentle caregiver view that reassures families without drowning them in notifications. But the heart of it is set: Willi is the friend who stays, listens, speaks your language, and gets help moving so being alone doesn\u2019t have to feel lonely.\\r\\n\\r\\n## Built With\\r\\n\\r\\n`fetch.ai`\\r\\n`freewili`\\r\\n`python``\\r\\n\\r\\n### Try it out\\r\\n\\r\\n<div class=\\"github-img\\">\\r\\n\\r\\n![GitHub](./github.png) <a href=\\"https://github.com/RunAns/HD4---Willi-Your-Care-Companion\\" target=\\"_blank\\"><span>GitHub Repo</span></a>\\r\\n</div>\\r\\n\\r\\n*Source* - [https://devpost.com/software/j-a-r-v-i-s-ek9pxu](https://devpost.com/software/j-a-r-v-i-s-ek9pxu)"},{"id":"agent-unblind","metadata":{"permalink":"/blog/agent-unblind","source":"@site/blog/2025-10-16/index.mdx","title":"Agent Unblind","description":"Agent Unblind handles sending email to customer, generating invoice, and take a photo and analyze the photo along with prompt, auto photo capturing system of thief upon our motion detection.","date":"2025-10-16T00:00:00.000Z","tags":[{"inline":false,"label":"FREE-WILi","permalink":"/blog/tags/FREE-WILi","description":"Make Embedded Systems Fun Again !"},{"inline":false,"label":"Hack Dearborn","permalink":"/blog/tags/hackdearborn","description":"hackdearborn"},{"inline":false,"label":"Student Project","permalink":"/blog/tags/studentproject","description":"studentproject"},{"inline":false,"label":"Hackathon","permalink":"/blog/tags/hackathon","description":"hackathon"}],"readingTime":13.92,"hasTruncateMarker":true,"authors":[{"name":"Han Lee","url":"https://devpost.com/skagen146","imageURL":"https://lh3.googleusercontent.com/a/ACg8ocJeR2m3z-u-D9wfbOVG0nu0QakDhHrlvWWzlWc7Azd5gEnsmj8=s96-c?height=180&width=180","key":"Han Lee","page":null}],"frontMatter":{"slug":"agent-unblind","title":"Agent Unblind","authors":["Han Lee"],"date":"2025-10-16T00:00:00.000Z","tags":["FREE-WILi","Hack Dearborn","student project","hackathon"]},"unlisted":false,"prevItem":{"title":"Willi: Your Care Companion","permalink":"/blog/willi-your-care-companion"},"nextItem":{"title":"M3SH","permalink":"/blog/m3sh"}},"content":"> Agent Unblind handles sending email to customer, generating invoice, and take a photo and analyze the photo along with prompt, auto photo capturing system of thief upon our motion detection.\\r\\n\\r\\nimport MyCarousel from \'@site/src/components/Carousel\';\\r\\n\\r\\n{/* <MyCarousel /> */}\\r\\n\\r\\nexport const slides = [\\r\\n  { type: \'video\', src: \'iZ00LBi73io\' }, // YouTube video\\r\\n  { type: \'image\', src: \'/img/blog/agent-unblind.jpg\', alt: \'agent-unblind\', caption: \\"\\" },\\r\\n];\\r\\n\\r\\n<MyCarousel slides={slides} />\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n<br/>\\r\\n\\r\\n## \u2728 Inspiration\\r\\n\\r\\nMore than 1.8 million business owners with disabilities in the U.S. often must overcome unique barriers to entrepreneurship as they strive to compete alongside their non-disabled counterparts in a business world that does not fully recognize their abilities and resilience.  \\r\\n\\r\\n**Source:** [National Disability Institute Report](https://www.nationaldisabilityinstitute.org/reports/small-business-ownership-pwd-challenges-and-opportunities/)\\r\\n\\r\\nI personally don\'t believe in \\"ability\\" and \\"disability\\". People that are blind or \\"disabled\\" just need a curated tool or pool of tools to enable themselves to do anything.  \\r\\nI would not call someone \\"disabled\\" just because he cannot stick a nail to the wall without a hammer. People generally need **tools** to do tasks.  \\r\\n\\r\\n\\"Disabled\\" or blind people specifically just need more customized tools. That\'s why I developed **Agent Unblind** that uses Text-to-Speech and Speech-to-Text to interact with the tool-calling/invoking agent built with **ASI LLM** as the reasoning brain that calls a chain of tools to do tasks.\\r\\n\\r\\nI built customized tools such as:\\r\\n- Signing up with credentials  \\r\\n- Logging in  \\r\\n- Getting my customer list  \\r\\n- Sending email to customer  \\r\\n- Generating invoice and sending an email with an invoice  \\r\\n- Using camera to analyze a photo that is taken after opening the camera  \\r\\n\\r\\nI also built a **theft detection system** with **FREE-WILi** that uses accelerometer data streamed to our Dashboard app through **WASM (Rust \u2192 wasm-bindgen)** and real-time data visualization of acceleration data of x, y, and z axes.\\r\\n\\r\\n## \ud83d\uded2 What it does\\r\\n\\r\\n- **Agent capabilities:**\\r\\n  - Sends email to customers  \\r\\n  - Generates invoices  \\r\\n  - Takes and analyzes photos along with prompts  \\r\\n  - Auto photo capturing system of thief upon motion detection  \\r\\n\\r\\n- **Motion detection:**\\r\\n  - Detects motion in a local WASM web app (port `7001`) and snaps a photo to `media/`.  \\r\\n  - Analyzes the most recent frame with **Gemini** to describe clothing, visible traits, and scene context (e.g., \u201c1 person; black hoodie; white sneakers; backpack; near the register\u201d).  \\r\\n  - Speaks alerts using **ElevenLabs TTS** (\u201cMotion detected. Theft alert!\u201d).  \\r\\n\\r\\n- **Agentic workflows via ASI:**\\r\\n  - Voice commands like:\\r\\n    - \u201cSign me up\u201d\\r\\n    - \u201cLog in\u201d\\r\\n    - \u201cAdd a customer\u201d\\r\\n    - \u201cEmail the incident\u201d\\r\\n    - \u201cAnalyze the latest frame\u201d\\r\\n  - The agent calls FastAPI tools automatically.  \\r\\n\\r\\n- **Customer & Auth Layer (MongoDB):**\\r\\n  - Sign up and log in (JWT)\\r\\n  - Create/list customers \u2014 scoped per user  \\r\\n\\r\\n- **Utilities:**\\r\\n  - Generate PDF invoices  \\r\\n  - Send emails with attachments  \\r\\n  - Run an ARM SME2 static analyzer (demo of domain tools)  \\r\\n\\r\\n## \ud83d\udee0 How we built it\\r\\n\\r\\n### Architecture Overview\\r\\n\\r\\n#### Frontends\\r\\n\\r\\n- **localhost:7001 (Flask/WASM)**  \\r\\n  Motion sensing, live capture, media hosting, and a `/capture_now` endpoint.  \\r\\n\\r\\n- **Agent Unblind UI**  \\r\\n  A simple Bootstrap page with mic capture that uploads audio to `/transcribe_and_run`, shows transcript + agent reply, and plays TTS.\\r\\n\\r\\n### Services\\r\\n\\r\\n- **localhost:8000 (FastAPI \u201cTools API\u201d)**  \\r\\n  Endpoints:\\r\\n  - `/tools/signup`\\r\\n  - `/tools/login`\\r\\n  - `/tools/get_myid` (JWT \u2192 user scope)\\r\\n  - `/tools/put_customer`\\r\\n  - `/tools/get_customer`\\r\\n  - `/tools/make_invoice`\\r\\n  - `/tools/send_email`\\r\\n  - `/tools/send_invoice_email`\\r\\n  - `/tools/camera_analyze` (webcam \u2192 Gemini)\\r\\n  - `/tools/analyze_thief_most_recent` (latest snapshot \u2192 Gemini)\\r\\n  - `/tools/arm_sme2/analyze` (mounted SME2 analyzer)\\r\\n\\r\\n- **JWT Auth:** HS256 with `iss/aud`, helper to derive `user_id` from Bearer token.  \\r\\n- **MongoDB:**  \\r\\n  - `users` collection (bcrypt hashes, unique indexes)  \\r\\n  - `customers` collection (scoped by user_id + email)\\r\\n\\r\\n### Agent Layer\\r\\n\\r\\n- **agent/asi_toolcaller.py**\\r\\n  - Sends chat + OpenAI-style tool schema to **ASI**\\r\\n  - Executes returned tool calls against port `:8000`\\r\\n  - Persists JWT in session and auto-injects JWT for scoped tools\\r\\n  - Exposes shim on port `:9100` for the voice UI  \\r\\n\\r\\n- **uAgents**\\r\\n  - Runs locally on port `:9200` with Agentverse inspector enabled\\r\\n  - Maintains conversation state and tool history\\r\\n  - Clean separation of concerns:\\r\\n    - **Agent** \u2192 policy + tool orchestration  \\r\\n    - **FastAPI service** \u2192 capabilities  \\r\\n\\r\\n### AI Integrations\\r\\n\\r\\n- **Gemini (google-genai)** for image understanding (JSON schema when possible)  \\r\\n- **ElevenLabs**\\r\\n  - **STT:** scribe_v1  \\r\\n  - **TTS:** multilingual v2  \\r\\n\\r\\n### Port Map\\r\\n\\r\\n| Port | Description                    |\\r\\n|------|--------------------------------|\\r\\n| 5000 | Main \u201cAgent Unblind\u201d Web app   |\\r\\n| 7001 | FREE-WILi WASM Dashboard App   |\\r\\n| 8000 | Tools API (FastAPI)            |\\r\\n| 9200 | uAgents internal server        |\\r\\n| 9100 | Local \u201cshim\u201d HTTP (optional)   |\\r\\n\\r\\nA small FastAPI endpoint (`POST /run_task`) is used to trigger the agent\u2019s ASI tool-calling loop via `curl` for development convenience.\\r\\n\\r\\n## \ud83e\udd16 Use of uAgents\\r\\n\\r\\n- Runs local agent (`asi-toolcaller`)  \\r\\n- Maintains conversation state and tool history  \\r\\n- Stores JWT + user_id after login and auto-injects for scoped calls  \\r\\n- Exposes message handler and lightweight HTTP shim for voice UI  \\r\\n- Clean separation of logic: agent (policy/orchestration) vs. service (capabilities)\\r\\n\\r\\n## \ud83d\udd79\ufe0f Use of Agentverse\\r\\n\\r\\nIntegration with **Agentverse inspector** for real-time observation of agent behavior and tool invocations.\\r\\n\\r\\n## \ud83e\udde0 Use of ASI\\r\\n\\r\\n- Calls `https://api.asi1.ai/v1/chat/completions`  \\r\\n- Uses tools schema (OpenAI-style) with `tool_choice=\\"auto\\"`  \\r\\n- Model used: **asi1-mini**  \\r\\n- Interprets natural language requests like:\\r\\n  - \u201cSign me up\u201d\\r\\n  - \u201cLog in\u201d\\r\\n  - \u201cAnalyze the latest scene\u201d  \\r\\n- Returns structured tool calls that are executed and fed back as `role:\\"tool\\"` turns  \\r\\n- Custom tools added:\\r\\n  - `signup`, `login`, `get_myid`\\r\\n  - `put_customer`, `get_customer`\\r\\n  - `analyze_thief_most_recent`\\r\\n\\r\\n## \ud83e\udd88 Use of FREE-WILi\\r\\n\\r\\n### Motion Sensor + WASM Dashboard + Voice Alert Overview\\r\\n\\r\\nA browser-based theft-detection dashboard that:\\r\\n\\r\\n- Reads live accelerometer data from a **FREE-WILi** device over Web Serial  \\r\\n- Processes signals in **Rust wasm-bindgen** for fast and consistent detection  \\r\\n- Visualizes data (`m/s\xb2` + EMA)  \\r\\n- Triggers **ElevenLabs TTS** alert \u2014  \\r\\n  \u201cMotion detected, theft alert!\u201d whenever movement crosses a threshold.\\r\\n\\r\\nClick Image to see Demo of Theft Prevention System powered by FREE-WILi and WASM\\r\\n[![Demo of Theft Prevention System](./agent-unblind-1.png)](https://www.youtube.com/watch?v=LX4KK7t_iYM)\\r\\n\\r\\n## \u2699\ufe0f Architecture (end-to-end)\\r\\n\\r\\nFREE-WILi streams raw accelerometer counts (x, y, z) and the current LSB-per-g scale over serial USB.\\r\\n\\r\\nBrowser (Web Serial API) reads those lines in real time.\\r\\n\\r\\nRust \u2192 WebAssembly (wasm-bindgen) converts counts \u2192 m/s\xb2, maintains an Exponential Moving Average (EMA) of |a|, and raises an alarm when |a| deviates from 1g by \u2265 threshold.\\r\\n\\r\\nDashboard (HTML/JS) plots ax, ay, az, |a|, EMA, updates status badges, and logs events.\\r\\n\\r\\nElevenLabs voice alert fires via a small /speak-motion endpoint (so the API key stays server-side) and plays back in the browser.\\r\\n\\r\\n## \ud83e\udde0 Motion detection logic (why it works)\\r\\n\\r\\nGravity at rest is ~1 g. Any significant motion changes the magnitude |a| = \u221a(ax\xb2 + ay\xb2 + az\xb2).\\r\\n\\r\\nConvert counts \u2192 g using the device\u2019s LSB per g, then g \u2192 m/s\xb2 by multiplying 9.80665.\\r\\n\\r\\nSmooth with EMA to reduce noise but keep responsiveness:\\r\\n\\r\\nema(t) = \u03b1 * |a|(t) + (1-\u03b1) * ema(t-1)\\r\\n\\r\\nAlarm if | |a| - 9.80665 | \u2265 thresh (e.g., 2.0 m/s\xb2).\\r\\n\\r\\n## \ud83e\udd80 Rust/WASM core\\r\\n\\r\\nRust gives deterministic math and great performance in the hot loop.\\r\\n\\r\\nBuild:\\r\\n\\r\\n```\\r\\ncargo install wasm-pack cd accel_wasm wasm-pack build --release --target web -d web/pkg\\r\\n\\r\\n```\\r\\n\\r\\n```\\r\\ncd accel_wasm python app.py --port 7001 --web-root web\\r\\n\\r\\n```\\r\\n\\r\\n## \ud83c\udf10 Browser data pipeline\\r\\n\\r\\nUser clicks \u201cConnect\u201d \u2192 Web Serial prompts for the FREE-WILi port.\\r\\n\\r\\nWe read newline-delimited frames like: x,y,z,lsb_per_g.\\r\\n\\r\\nFeed each frame into Detector.process_counts(...).\\r\\n\\r\\nUpdate the chart + status; if alarm=true, call /speak-motion?text=Motion%20detected%2C%20theft%20alert!.\\r\\n\\r\\n## \ud83d\udde3\ufe0f Use of ElevenLabs\\r\\n\\r\\nTheft Detection Voice is done with ElevenLabs and the Text-to-Speech and Speech-to-Text in the Main Agent Unblind app is also done with ElevenLabs.\\r\\n\\r\\n## \ud83d\udcbb Use of ARM\\r\\n\\r\\n![Use of ARM](./agent-unblind-2.png \\"Use of ARM\\")\\r\\n![Use of ARM](./agent-unblind-3.png \\"Use of ARM\\")\\r\\n\\r\\n**SME2 static analyzer**: We built a FastAPI microservice that compiles tiny AArch64 kernels with LLVM clang (Armv9-A target) and evaluates them with llvm-mca to get IPC and block throughput. It compares a na\xefve GEMM baseline against an SME/SME2 streaming-mode variant, returning a JSON report plus optional raw MCA output for deep dives.\\r\\n\\r\\n**Neoverse/Cortex targeting**: The service takes -mcpu (e.g., neoverse-v2, neoverse-n2, cortex-x4) so we can see how the same kernel schedules across Arm cores. This helped us reason about expected gains and bottlenecks before touching real hardware.\\r\\n\\r\\n**Armv9-A & SVE2 path**: Alongside the baseline and SME2 stub, we added an autovectorized SVE2 variant (using -march=armv9-a+sve2 and optional -msve-vector-bits=) to compare baseline vs SVE2 vs SME2 side-by-side.\\r\\n\\r\\n**Streaming mode + ZA plumbing**: We explicitly enter/exit SME streaming mode and toggle ZA (smstart sm/za \u2026 smstop za/sm) to validate state handling and ensure our future SME2 micro-kernels will model correctly.\\r\\n\\r\\n**Developer workflow on macOS/Linux**: We used Homebrew LLVM (clang/llvm-mca 21.x) with -target aarch64-linux-gnu for cross-assembly generation, making it easy to test Arm scheduling locally without an Arm board.\\r\\n\\r\\n**Tooling as an agent tool**: The analyzer is exposed as /tools/arm_sme2/analyze and our uAgents agent can call it in chains (e.g., \u201crun SME2 analyzer mcpu=neoverse-v2 n=256\u201d), turning Arm perf exploration into a composable tool inside the broader agent system.\\r\\n\\r\\n**Actionable output**: The endpoint returns:\\r\\n\\r\\nipc, block_rthroughput, and (optionally) top resource pressures\\r\\n\\r\\nA computed predicted speedup vs. baseline\\r\\n\\r\\nOptional raw llvm-mca text for profiling/port pressure inspection\\r\\n\\r\\n### Example call:\\r\\n\\r\\n```bash\\r\\ncurl -s -X POST http://127.0.0.1:8000/tools/arm_sme2/analyze \\\\\\r\\n  -H \\"Content-Type: application/json\\" \\\\\\r\\n  -d \'{\\"n\\":256,\\"mcpu\\":\\"neoverse-v2\\",\\"march\\":\\"armv9-a+sve2\\",\\"include_raw_mca\\":false}\' \\\\\\r\\n| python -m json.tool\\r\\n\\r\\n```\\r\\n\\r\\n**TLDR**\\r\\n\\r\\nThis tool is comparing different implementations of GEMM (General Matrix Multiply):\\r\\n\\r\\n* Baseline (na\xefve C code) \u2014 simple nested loops, scalar multiply-add.\\r\\n* SVE2 variant \u2014 auto-vectorized or hand-tuned for scalable vector extensions.\\r\\n* SME/SME2 variant \u2014 uses Arm\u2019s Streaming Matrix Extension, which provides a 2D register file (ZA) and streaming mode for block-matrix math.\\r\\n\\r\\nThis lets you see how much faster SME2 can theoretically execute GEMM compared to the baseline or SVE2 versions.\\r\\n\\r\\nThis tool can be called from our agent.\\r\\n\\r\\n![tool](./agent-unblind-4.png \\"tool\\")\\r\\n\\r\\n## \ud83e\udde0 Use of Gemini\\r\\n\\r\\nWe built a tool using Gemini 2.5 that takes a prompt along with an image and gives response from Gemini. Agent is calling this tool to open a camera and take a photo and convert it into base64 to get response when user prompt asks image analysis.\\r\\n\\r\\nAlso, we built thief_analyzer tool that uses the most recent photo taken when the FREE-WILi sensor detected motion/acceleration. The Theft detection powered by FREE-WILi promptly opens a camera and take a photo when detecting motion. thief_analyzer tool takes the most recent photo to the Gemini for analysis, you can give prompt such as \\"describe the person in the thief photo and his physical appearance\\".\\r\\n\\r\\n## \ud83d\udcbb Use of Gemini 2.5 Computer Use\\r\\n\\r\\n![Gemini 2.5](./agent-unblind-5.png \\"Gemini 2.5\\")\\r\\n\\r\\nMODEL = \\"gemini-2.5-computer-use-preview-10-2025\\"\\r\\n\\r\\n## \ud83e\udde0 What We Built\\r\\n\\r\\nWe experimented with Google Gemini 2.5 Computer Use, a new agentic model that can literally see and control user interfaces. Our goal was to let Gemini autonomously interact with our FREE-WILi Theft Detection WASM Dashboard (served at http://localhost:7001/).\\r\\n\\r\\nIn short:\\r\\n\\r\\n\ud83e\udde9 We wanted Gemini 2.5 Computer Use to open the dashboard, click \u201cConnect Device\u201d, select the correct g-range, read live motion data, and trigger a voice alert via ElevenLabs when motion was detected \u2014 all automatically.\\r\\n\\r\\n## \u2699\ufe0f How It Was Set Up\\r\\n\\r\\nWe wrote two Python entry points:\\r\\n\\r\\n### main.py\\r\\n\\r\\nA full Computer Use agent loop built from Google\u2019s reference code. It used:\\r\\n\\r\\n- google-genai client to talk to the model gemini-2.5-computer-use-preview-10-2025\\r\\n- Playwright to actually perform the clicks, typing, and screenshot capture\\r\\n- Flask backend serving the local dashboard and /speak-motion endpoint (for TTS alerts)\\r\\n\\r\\nLoop flow:\\r\\n\\r\\n1. Send Gemini a goal prompt and a screenshot of the web UI.\\r\\n2. Receive the model\u2019s function_call (e.g., click_at, type_text_at).\\r\\n3. Execute those actions in Playwright.\\r\\n4. Capture a new screenshot and feed it back to the model.\\r\\n5. Repeat until Gemini completes the task.\\r\\n\\r\\nEssentially, main.py let Gemini become a test engineer for our IoT dashboard.\\r\\n\\r\\n### tiny.py\\r\\n\\r\\nA minimal proof-of-concept version that runs a single iteration \u2014 the smallest possible working example of Computer Use. It only asked Gemini to:\\r\\n\\r\\n> \u201cOpen http://localhost:7001/ and click Connect Device.\u201d\\r\\n\\r\\nNo complex safety or confirmation handling, just one round-trip to show the API flow.\\r\\n\\r\\n## \ud83d\udea7 What Happened\\r\\n\\r\\nThe code executed correctly up to the API call.\\r\\n\\r\\nBut our Google AI Studio project had no free quota for the Computer Use Preview model, so the API returned 429 RESOURCE_EXHAUSTED.\\r\\n\\r\\nBecause of that, Gemini never generated real UI actions \u2014 we could only test the Playwright execution layer locally.\\r\\n\\r\\n## \ud83e\udde9 Fallback Solution\\r\\n\\r\\nWhile waiting for access, we built a local simulation using Playwright only (no API). It clicks the same buttons and reads the same DOM elements, mimicking what Gemini 2.5 Computer Use would do once quota is enabled.\\r\\n\\r\\n## \ud83d\udca1 Summarize\\r\\n\\r\\nWe integrated Google\u2019s new Gemini 2.5 Computer Use model to let an AI agent control our FREE-WILi WebAssembly motion-sensor dashboard in real time. Using Python (Flask + Playwright) and Gemini\u2019s Computer Use API, the agent was designed to open the dashboard, connect the sensor, monitor live motion data, and trigger ElevenLabs voice alerts. Although our current project ran into preview-quota limits, the full loop demonstrates how Gemini can autonomously interact with real web interfaces \u2014 an important step toward hands-free IoT control agents.\\r\\n\\r\\n## \ud83e\udde9 What We Built with auto_connect.py\\r\\n\\r\\nWe developed an automation script using Playwright to demonstrate the same capabilities that the Gemini 2.5 Computer Use API provides \u2014 but running locally and for free.\\r\\n\\r\\nOur goal was to show how an intelligent agent could control a real browser to interact with our FREE-WILi WebAssembly Motion Sensor Dashboard, a live page served at http://localhost:7001/.\\r\\n\\r\\n## \u2699\ufe0f What the Script Does\\r\\n\\r\\nauto_connect.py automatically:\\r\\n\\r\\n- Launches Google Chrome in a controlled environment\\r\\n- Uses a persistent Chrome profile so that WebSerial permissions (for the hardware sensor) are remembered across sessions.\\r\\n\\r\\n  This simulates the secure sandboxed environment required by Gemini Computer Use.\\r\\n\\r\\n- Opens the local dashboard (http://localhost:7001/)\\r\\n\\r\\n  The same page where the motion sensor streams real-time data from the FREE-WILi board.\\r\\n\\r\\n- Clicks the \u201cConnect Device\u201d button automatically\\r\\n\\r\\n  The first time you run it, you manually select the serial port (DisplayCPU v62) once. After that, the permission is remembered and the script connects without showing the Chrome serial-port chooser again.\\r\\n\\r\\n- Selects g_range = 16384 (\xb1 2 g) from the dropdown\\r\\n\\r\\n  Ensures the accelerometer scale is correct for motion detection.\\r\\n\\r\\n- Reads the live sensor status directly from the page DOM\\r\\n\\r\\n  Prints values such as:\\r\\n\\r\\n  - Connection Status\\r\\n  - Motion Status\\r\\n  - EMA (m/s\xb2)\\r\\n  - |a| (m/s\xb2)\\r\\n  - X/Y/Z (m/s\xb2)\\r\\n\\r\\nThis verifies that the browser and the sensor are communicating correctly.\\r\\n\\r\\n## \ud83e\udde0 What We Demonstrated\\r\\n\\r\\nThis script simulates Gemini 2.5 Computer Use locally:\\r\\n\\r\\nInstead of the Gemini API issuing click_at or type_text_at actions, Playwright performs those actions on a real Chrome browser.\\r\\n\\r\\nWe showed how an autonomous agent can see, act, and read back from a GUI \u2014 the same closed-loop logic described in Google\u2019s official Computer Use documentation.\\r\\n\\r\\n## \ud83d\udea7 Challenges we ran into\\r\\n\\r\\n- One struggle was the lack of quota on Gemini 2.5 Computer Use with Gemini Free Tier. I implemented the code using \\"gemini-2.5-computer-use-preview-10-2025\\" that auto-press the button \\"Connect Button\\" of Theft Detection WASM Dashboard app, since clicking would be burdensome for our target user of blind people.\\r\\n- We provided alternative using playwright to automate the clicking button, but playwright was not native to chrome, so it only does auto-click not selecting port.\\r\\n- We ran out of free credit from ElevenLabs and made another account.\\r\\n- I gave out my ESP camera to someone in need. Later I realized my theft detection WASP app uses my webcam and my main Agent Unblind app also has a tool for opening a camera and taking a photo to send it to Gemini. The two apps cannot share the camera; I have to turn one off and turn another on.\\r\\n\\r\\n## \ud83c\udfc6 Accomplishments that we\'re proud of\\r\\n\\r\\n- We are proud to have implemented 1 of 9 learning paths that were available for our MacOS.\\r\\n- We are also proud to have implemented the Gemini 2.5 Computer Use since this was new.\\r\\n- We are also proud to utilize FREE-WILi.\\r\\n\\r\\n## \ud83d\udcda What we learned\\r\\n\\r\\n- We learned a lot, especially the ARM learning path.\\r\\n- We are proud to have implemented 1 of 9 learning paths that were available for our MacOS.\\r\\n\\r\\n## \ud83d\ude80 What\'s next for Agent Unblind\\r\\n\\r\\n- We need to build more customized tools for our Tool Calling/Invoking Agent to utilize to benefit blind people.\\r\\n- It would be better if we develop a mobile version, as well.\\r\\n\\r\\n## \ud83e\udde9 Built With  \\r\\n\\r\\n```arm```\\r\\n```c```\\r\\n```dotenv```\\r\\n```fastapi```\\r\\n```flask```\\r\\n```free-wili```\\r\\n```html```\\r\\n```javascript```\\r\\n```jwt```\\r\\n```mongodb```\\r\\n```pydantic```\\r\\n```pymongo```\\r\\n```python```\\r\\n```rust```\\r\\n```uagents```\\r\\n```wasm```\\r\\n```wasm-bindgen```\\r\\n\\r\\n### Try it out\\r\\n\\r\\n<div class=\\"github-img\\">\\r\\n\\r\\n![GitHub](./github.png) <a href=\\"https://github.com/hlee18lee46/hackdb\\" target=\\"_blank\\"><span>GitHub Repo</span></a>\\r\\n</div>\\r\\n\\r\\n<div class=\\"github-img\\">\\r\\n\\r\\n![GitHub](./youtube.png) <a href=\\"https://www.youtube.com/watch?v=LX4KK7t_iYM\\" target=\\"_blank\\"><span>YouTube</span></a>\\r\\n</div>\\r\\n\\r\\n\\r\\n*Source* - [https://devpost.com/software/agent-unblind](https://devpost.com/software/agent-unblind)"},{"id":"m3sh","metadata":{"permalink":"/blog/m3sh","source":"@site/blog/2025-10-16/m3sh.mdx","title":"M3SH","description":"M3SH is secure, real-time radio triangulation that saves lives.","date":"2025-10-16T00:00:00.000Z","tags":[{"inline":false,"label":"FREE-WILi","permalink":"/blog/tags/FREE-WILi","description":"Make Embedded Systems Fun Again !"},{"inline":false,"label":"Hack Dearborn","permalink":"/blog/tags/hackdearborn","description":"hackdearborn"},{"inline":false,"label":"Student Project","permalink":"/blog/tags/studentproject","description":"studentproject"},{"inline":false,"label":"Hackathon","permalink":"/blog/tags/hackathon","description":"hackathon"}],"readingTime":2.92,"hasTruncateMarker":true,"authors":[{"name":"Forest Qin","url":"https://devpost.com/forestq24","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Forest Qin","page":null},{"name":"Rupesh Kanna Kaviyasree Narayanan","url":"https://devpost.com/buffbiryani","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Anish Nandamuri","page":null}],"frontMatter":{"slug":"m3sh","title":"M3SH","authors":["Forest Qin","Anish Nandamuri"],"date":"2025-10-16T00:00:00.000Z","tags":["FREE-WILi","Hack Dearborn","student project","hackathon"]},"unlisted":false,"prevItem":{"title":"Agent Unblind","permalink":"/blog/agent-unblind"},"nextItem":{"title":"MeyesAI","permalink":"/blog/meyesai"}},"content":"> M3SH is secure, real-time radio triangulation that saves lives.\\r\\n\\r\\nimport MyCarousel from \'@site/src/components/Carousel\';\\r\\n\\r\\n{/* <MyCarousel /> */}\\r\\n\\r\\nexport const slides = [\\r\\n  { type: \'video\', src: \'BklTSwI_pO4\' },\\r\\n];\\r\\n\\r\\n<MyCarousel slides={slides} />\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n<br/>\\r\\n\\r\\n## Inspiration\\r\\n\\r\\nWhen communication lines fail, lives are put at risk. Natural disasters, battlefield chaos, and wilderness rescues all share one critical challenge: connectivity failure. **M3SH** was born from the need for resilient communication when the internet, satellites, and cell towers go dark. Inspired by tactical radio systems and swarm intelligence, we set out to build a **self-healing mesh network** that keeps people connected anywhere.\\r\\n\\r\\n## What it does\\r\\n\\r\\n**M3SH** creates an autonomous, offline communication network using embedded radio nodes.\\r\\n\\r\\n- Each node acts as both a **transmitter** and **receiver**\\r\\n- Nodes relay messages through nearby nodes to form a **self-healing mesh**\\r\\n- Works **without the Internet**, **Wi-Fi**, **or** **satellites**\\r\\n- Ideal for **military operations, disaster relief, and search-and-rescue missions**\\r\\n- Displays real-time **signal strength (RSSI)** and **relay paths** for situational awareness\\r\\n- M3SH transforms isolation into connection, enabling secure coordination when everything else fails.\\r\\n\\r\\n## How we built it\\r\\n\\r\\n- **Hardware**: Free-WiLi modules (ESP32-based) for node-to-node radio communication\\r\\n- **Firmware**: FREE-WILi Python environment for signal broadcast, relay, and triangulation logic\\r\\n- **Frontend**: React + Next.js + TypeScript dashboard to visualize live nodes, packet flow, and signal maps\\r\\n- **Backend**: Node.js server to log node telemetry and packet routes\\r\\n- **Agents**: Lightweight AI processes simulate network optimization and adaptive rerouting\\r\\n\\r\\n## Challenges we ran into\\r\\n\\r\\n- **FREE-WILi**: a major challenge we ran into was the lack of macOS support for FREE-WILi development. Since all of our team used Macs, we struggled to install the necessary drivers and toolchains to flash and configure the devices. So we brought in Windows machines, which had built-in support.\\r\\n- **Bottlenose modul**: we ran into several issues with the Bottlenose module while initializing the Display and Main CPUs. After testing, we found newer firmware versions were incompatible with our setup. Rolling both back finally let us load the Bottlenose drivers and restore stable Wi-Fi and Bluetooth access.\\r\\n- **Signal calibration**: translating RSSI values into accurate distance metrics for triangulation using the standard log-distance path loss model.\\r\\n- **Network synchronization**: ensuring nodes forward packets without infinite loops or message loss\\r\\n- **Visualization**: rendering live mesh changes in real time without lag\\r\\n- **Hardware constraints**: limited memory and CPU on FREE-WILi boards restricted AI features\\r\\n\\r\\n## Accomplishments that we\'re proud of\\r\\n\\r\\n- Built a functioning **offline mesh network** using inexpensive FREE-WILi hardware\\r\\n- Achieved **real-time multi-hop packet routing** with visual relay tracing\\r\\n- Designed a **signal-to-distance mapping system** for node triangulation and localization\\r\\n- Created a working **command-and-control dashboard** for monitoring and testing the network\\r\\n\\r\\n## What we learned\\r\\n\\r\\n- How to translate **radio frequency (RF)** principles into practical mesh communication\\r\\n- The balance between **range, packet loss, and power efficiency** in embedded systems\\r\\n- How to use **RSSI, TTL, and latency** data to optimize routing paths\\r\\n- Agents like Fetch.ai are powerful tools for **futuristic** automation.\\r\\n\\r\\n## What\'s next for our Team\\r\\n\\r\\n- **Drone integration**: airborne relays to expand mesh range over disaster zones\\r\\n- **Encryption**: secure message transmission for military-grade safety\\r\\n- **Offline AI agents**: local models that adjust node power, frequency, and routing autonomously\\r\\n- **Multi-device compatibility**: smartphone or wearable access to the mesh via BLE bridges\\r\\n- **Global scalability**: adapting M3SH for international disaster response and field deployments\\r\\n\\r\\n## Built With\\r\\n\\r\\n`agentverse`\\r\\n`cloudflare`\\r\\n`fetch.ai`\\r\\n`free-wili`\\r\\n`next.js`\\r\\n`node.js`\\r\\n`python`\\r\\n`react`\\r\\n`rest-api`\\r\\n`shadcn/ui`\\r\\n`tailwindcss`\\r\\n`typescript`\\r\\n`uagent`\\r\\n`vercel`\\r\\n`vite`\\r\\n\\r\\n*Source* - [https://devpost.com/software/m3sh-vhqy3j](https://devpost.com/software/m3sh-vhqy3j)"},{"id":"meyesai","metadata":{"permalink":"/blog/meyesai","source":"@site/blog/2025-10-16/meyesai.mdx","title":"MeyesAI","description":"Restoring more than vision.","date":"2025-10-16T00:00:00.000Z","tags":[{"inline":false,"label":"FREE-WILi","permalink":"/blog/tags/FREE-WILi","description":"Make Embedded Systems Fun Again !"},{"inline":false,"label":"Hack Dearborn","permalink":"/blog/tags/hackdearborn","description":"hackdearborn"},{"inline":false,"label":"Student Project","permalink":"/blog/tags/studentproject","description":"studentproject"},{"inline":false,"label":"Hackathon","permalink":"/blog/tags/hackathon","description":"hackathon"}],"readingTime":4.33,"hasTruncateMarker":true,"authors":[{"name":"Adrian Tlatelpa","url":"https://devpost.com/hh5426","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Adrian Tlatelpa","page":null},{"name":"Blaine Oania","url":"https://devpost.com/blaine-oania","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Blaine Oania","page":null},{"name":"Jaideep Siva Senthilprabhakar","url":"https://devpost.com/jaideep-siva","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Jaideep Siva Senthilprabhakar","page":null},{"name":"Rupesh Kanna Kaviyasree Narayanan","url":"https://devpost.com/rupkan05","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Rupesh Kanna Kaviyasree Narayanan","page":null}],"frontMatter":{"slug":"meyesai","title":"MeyesAI","authors":["Adrian Tlatelpa","Blaine Oania","Jaideep Siva Senthilprabhakar","Rupesh Kanna Kaviyasree Narayanan"],"date":"2025-10-16T00:00:00.000Z","tags":["FREE-WILi","Hack Dearborn","student project","hackathon"]},"unlisted":false,"prevItem":{"title":"M3SH","permalink":"/blog/m3sh"},"nextItem":{"title":"Tracker Detector","permalink":"/blog/tracker-detector"}},"content":"> Restoring more than vision.\\r\\n\\r\\nimport MyCarousel from \'@site/src/components/Carousel\';\\r\\n\\r\\n{/* <MyCarousel /> */}\\r\\n\\r\\nexport const slides = [\\r\\n  { type: \'image\', src: \'/img/blog/meyesai.jpg\', alt: \'meyesai\', caption: \\"Robot dog sensor visualization with annotated images.\\" },\\r\\n  { type: \'image\', src: \'/img/blog/meyesai-2.jpg\', alt: \'meyesai\', caption: \\"Monodepth model.\\" },\\r\\n  { type: \'image\', src: \'/img/blog/meyesai-3.jpg\', alt: \'meyesai\', caption: \\"\\" },\\r\\n  { type: \'image\', src: \'/img/blog/meyesai-4.jpg\', alt: \'meyesai\', caption: \\"\\" },\\r\\n  { type: \'image\', src: \'/img/blog/meyesai-5.jpg\', alt: \'meyesai\', caption: \\"\\" },\\r\\n  { type: \'image\', src: \'/img/blog/meyesai-6.jpg\', alt: \'meyesai\', caption: \\"\\" },\\r\\n  { type: \'image\', src: \'/img/blog/meyesai-7.jpg\', alt: \'meyesai\', caption: \\"\\" },\\r\\n];\\r\\n\\r\\n<MyCarousel slides={slides} />\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n<br/>\\r\\n\\r\\n## MeyeAI: AI Intelligence for Assistive Navigation\\r\\n\\r\\nWe built an AI that gives any camera the spatial intelligence of a guide dog - turning smartphones, robots, or smart glasses into personalized navigation assistants for the visually impaired.\\r\\n\\r\\n## The Problem We Solved\\r\\n\\r\\n1.3 billion people worldwide live with vision impairment. While guide dogs cost $50,000+ and take years to train, and white canes only detect ground-level obstacles, there\'s a massive gap in affordable, intelligent navigation assistance.\\r\\n\\r\\n## Our Solution: Beyond Hardware - A Spatial AI Platform\\r\\n\\r\\nWe developed an adaptable intelligence system that transforms any camera-equipped device into an assistive navigation companion. During the hackathon, we demonstrated this through:\\r\\n\\r\\n* **Monocular Depth Estimation**: Our AI extracts 3D spatial information from a single camera\u2014no expensive LiDAR needed - making it deployable on everything from $50 smartphones to autonomous robots\\r\\n* **Real-time Semantic Understanding**: We fine-tuned vision models to identify and track 100+ everyday objects, providing contextual awareness like \\"coffee table 3 feet ahead, doorway to your right\\"\\r\\n* **Vision-Language Model Integration**: Our VLM can describe complex scenes beyond simple object detection\u2014understanding activities (\\"someone is cooking\\"), reading text in the environment, and providing rich contextual descriptions of spaces\\r\\n* **Natural Language Navigation**: Our LLM integration doesn\'t just detect objects\u2014it understands intent. Users can ask \\"Where\'s the bathroom?\\" or \\"Find me a seat\\" and get intelligent guidance\\r\\n* **Responsive and Adaptive Feedback System**: Spatial audio cues and conversational responses that adapt to each user\'s preferences\\r\\n\\r\\n## Technical Implementation\\r\\n\\r\\nWe built three proof-of-concepts in 24 hours:\\r\\n\\r\\n* **Quadruped Robot Demo**: Showcased autonomous navigation with real-time environmental understanding\\r\\n* **Web Application**: Demonstrated smartphone compatibility, turning any phone into an assistive device\\r\\n* **API Framework**: Proved our system can integrate with existing assistive tech, smart glasses, or IoT devices\\r\\n\\r\\n## Our Key Differentiators\\r\\n\\r\\n- Magnitudes cheaper\\r\\n- Instantly deployable\\r\\n- Continuously learning\\r\\n- Platform-agnostic\\r\\n\\r\\n## Impact Beyond Accessibility\\r\\n\\r\\nWhile we\'re starting with visual impairment, Avis\'s spatial AI has applications in:\\r\\n\\r\\n- Warehouse robotics for inventory navigation\\r\\n- Elder care for fall prevention and medication reminders\\r\\n- Search and rescue operations in low-visibility conditions\\r\\n- Augmented reality gaming and education\\r\\n\\r\\n## How We Built It\\r\\n\\r\\nWe architected Avis as a modular system with three core layers:\\r\\n\\r\\n1. **Perception Layer**: We developed multi-functional computer vision models combining monocular depth and VLMs. We also implemented  state-of-the-art YOLO models for real-time object detection, optimizing them to run efficiently on edge devices\\r\\n2. **Intelligence Layer**: We connected LLMs for natural language understanding and created custom prompting strategies to translate visual data into helpful, context-aware guidance\\r\\n3. **Interface Layer**: We built ROS2 integration for the quadruped robot, a responsive React web app with WebRTC for real-time video streaming, and a RESTful API for third-party integrations\\r\\n\\r\\nOur tech stack included Python for AI model deployment, JavaScript/React for the web interface, and ROS2 for robot control\u2014all orchestrated through Docker containers for easy deployment.\\r\\n\\r\\n## Challenges We Faced\\r\\n\\r\\n* **Real-time Processing on Edge Devices**: Balancing model accuracy with inference speed was crucial. We had to optimize our models to run at 15+ FPS on mobile processors while maintaining reliable object detection\\r\\n* **Sensor Alignment and Calibration**: Getting accurate depth estimation from a moving robot required complex coordinate transformations and real-time calibration adjustments.\\r\\n* **Natural Language Generation**: Creating descriptions that were informative but not overwhelming required careful prompt engineering and user testing to find the right balance.\\r\\n* **Local v. Cloud**: The demanding requirements of real-time responsiveness required precise benchmarking of round-trip response times. ## What We Learned\\r\\n* **The Power of Monocular Depth**: We discovered that modern AI can extract surprisingly accurate 3D information from single cameras, democratizing spatial computing\\r\\n* **Accessibility is About Choice**: Through user feedback, we learned that different users want different levels of detail\u2014some want just critical obstacles, others want rich scene descriptions\\r\\n* **Edge Computing is Essential**: For real-time assistance, we had to move core processing to the device itself, teaching us valuable lessons about model optimization and quantization\\r\\n* **Interdisciplinary Design Matters**: Building for accessibility required us to think beyond technical metrics and consider human factors like cognitive load, alert fatigue, and personal autonomy\\r\\n\\r\\nMost importantly, we learned that the best assistive technology doesn\'t replace human judgment, it enhances it by providing the right information at the right time.\\r\\n\\r\\n## Next Steps\\r\\n\\r\\nWe\'re looking to continue the development of MeyesAI by partnering with local vision impairment organizations to refine our feedback systems with the goal of returning agency to visually impaired people around the world.\\r\\n\\r\\n## Built With\\r\\n\\r\\n`huggingface`\\r\\n`javascript`\\r\\n`llm`\\r\\n`oakd`\\r\\n`opencv`\\r\\n`python`\\r\\n`pytorch`\\r\\n`react`\\r\\n`ros2`\\r\\n`unitree`\\r\\n``vlm``\\r\\n\\r\\n*Source* - [https://devpost.com/software/avis-0wal3o](https://devpost.com/software/avis-0wal3o)"},{"id":"tracker-detector","metadata":{"permalink":"/blog/tracker-detector","source":"@site/blog/2025-10-16/tracker-detector.mdx","title":"Tracker Detector","description":"Your personal radar for AirTags, Tiles, and everything sneaky.","date":"2025-10-16T00:00:00.000Z","tags":[{"inline":false,"label":"FREE-WILi","permalink":"/blog/tags/FREE-WILi","description":"Make Embedded Systems Fun Again !"},{"inline":false,"label":"Hack Dearborn","permalink":"/blog/tags/hackdearborn","description":"hackdearborn"},{"inline":false,"label":"Student Project","permalink":"/blog/tags/studentproject","description":"studentproject"},{"inline":false,"label":"Hackathon","permalink":"/blog/tags/hackathon","description":"hackathon"}],"readingTime":0.97,"hasTruncateMarker":true,"authors":[{"name":"Ahmad Hashem","url":"https://devpost.com/ahashemm","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Ahmad Hashem","page":null},{"name":"Mohamad Syaj","url":"https://devpost.com/mbsyaj","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Mohamad Syaj","page":null},{"name":"Jad Jonaidi","url":"https://devpost.com/jadjon","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Jad Jonaidi","page":null},{"name":"Haitham Hayajneh","url":"https://devpost.com/hayajneh","imageURL":"https://img.icons8.com/?size=100&id=0C05f55sN8hD&format=png&color=000000","key":"Haitham Hayajneh","page":null}],"frontMatter":{"slug":"tracker-detector","title":"Tracker Detector","authors":["Ahmad Hashem","Mohamad Syaj","Jad Jonaidi","Haitham Hayajneh"],"date":"2025-10-16T00:00:00.000Z","tags":["FREE-WILi","Hack Dearborn","student project","hackathon"]},"unlisted":false,"prevItem":{"title":"MeyesAI","permalink":"/blog/meyesai"},"nextItem":{"title":"\ud83d\udc4b Gestura: Gesture-Based Mouse & Voice Control","permalink":"/blog/gestura"}},"content":"> Your personal radar for AirTags, Tiles, and everything sneaky.\\r\\n\\r\\nInspiration We kept hearing about hidden tags in backpacks, cars, and dorms. We wanted a simple, student-friendly way to see what\u2019s around you with just a Raspberry Pi. \x3c!-- truncate --\x3e What it does Scans BLE beacons and flags tracker-like signals (AirTag/Find My, Tile, SmartTag, Pebblebee). Uses RSSI smoothing, dwell time, and Apple-specific payload checks to cut false alarms. Optional Enroll mode fingerprints your demo tag. How we built it Python + Bleak on a Pi. Async scanning loop, moving-average RSSI, per-device timers/cooldowns, iBeacon exclusion, tight Find-My heuristics, LED blink on alert. Challenges we ran into Apple\u2019s 0x004C is everywhere, MACs rotate, and hackathon halls are noisy. We fixed it with stricter payload checks, persistence windows, and rate limiting. Accomplishments we\u2019re proud of It actually works in a crowded room, the Enroll mode makes demos clean, and everything runs locally\u2014no cloud, no accounts. What we learned BLE ads > MACs, signal smoothing matters, and small UX touches (LED blink, clear logs) build trust.\\r\\n\\r\\n## Built With\\r\\n\\r\\n`python`\\r\\n`raspberry-pi`\\r\\n\\r\\n*Source* - [https://devpost.com/software/tracker-detector](https://devpost.com/software/tracker-detector)"},{"id":"gestura","metadata":{"permalink":"/blog/gestura","source":"@site/blog/2025-10-07/gestura.mdx","title":"\ud83d\udc4b Gestura: Gesture-Based Mouse & Voice Control","description":"Gesture-based mouse built using FREE-WILi and Vosk text-to-speech used to increase accessibility in using the web for disabled people, such as amputees and people with lost-mobility in their hands.","date":"2025-10-07T00:00:00.000Z","tags":[{"inline":false,"label":"FREE-WILi","permalink":"/blog/tags/FREE-WILi","description":"Make Embedded Systems Fun Again !"},{"inline":false,"label":"Mhacks","permalink":"/blog/tags/mhacks","description":"mhacks"},{"inline":false,"label":"Student Project","permalink":"/blog/tags/studentproject","description":"studentproject"},{"inline":false,"label":"Hackathon","permalink":"/blog/tags/hackathon","description":"hackathon"}],"readingTime":3.13,"hasTruncateMarker":true,"authors":[{"name":"Chirag Bhat","url":"https://devpost.com/cbhat","imageURL":"https://d112y698adiu2z.cloudfront.net/photos/production/user_photos/003/251/912/datas/profile.jpeg","key":"Chirag Bhat","page":null},{"name":"Zayn Baig","url":"https://devpost.com/zayn464","imageURL":"https://lh3.googleusercontent.com/a/ACg8ocJ_p90iXvOh2Q09ZoE22pGigN3_hIApkK5QDfUII9FbmVw4AVr6=s96-c?height=180&width=180","key":"Zayn Baig","page":null}],"frontMatter":{"slug":"gestura","title":"\ud83d\udc4b Gestura: Gesture-Based Mouse & Voice Control","authors":["Chirag Bhat","Zayn Baig"],"date":"2025-10-07T00:00:00.000Z","tags":["FREE-WILi","mhacks","student project","hackathon"]},"unlisted":false,"prevItem":{"title":"Tracker Detector","permalink":"/blog/tracker-detector"},"nextItem":{"title":"celestaisle","permalink":"/blog/celestaisle"}},"content":"> Gesture-based mouse built using FREE-WILi and Vosk text-to-speech used to increase accessibility in using the web for disabled people, such as amputees and people with lost-mobility in their hands.\\r\\n\\r\\nimport MyCarousel from \'@site/src/components/Carousel\';\\r\\n\\r\\n{/* <MyCarousel /> */}\\r\\n\\r\\nexport const slides = [\\r\\n  { type: \'video\', src: \'7x3D7nPJlRc\' }, // YouTube video\\r\\n  { type: \'image\', src: \'/img/blog/gestura.jpg\', alt: \'gestura\', caption: \\"Gestura: Gesture-Based Mouse & Voice Control\\" },\\r\\n];\\r\\n\\r\\n<MyCarousel slides={slides} />\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n<br/>\\r\\n\\r\\n## \ud83d\udca1 Inspiration\\r\\n\\r\\nAt the\xa0**FREE-WILi workshop**\xa0held during\xa0**MHacks**\xa0this year, our team was inspired by a demo of a\xa0**Theremin-like device**. It used hand gestures\u2014detected via the FREE-WILi\u2019s accelerometer\u2014to control pitch and volume. This sparked an idea: what if we could harness similar gestures for\xa0**accessibility**?\\r\\n\\r\\nOne of our teammates has a family member who suffers from\xa0**severe Rheumatoid Arthritis**, which makes traditional mouse and keyboard usage extremely difficult. We envisioned a system that would allow users to control a computer using\xa0**just hand gestures and voice commands**\u2014no physical touch required.\\r\\n\\r\\nMoreover, Gestura, which can be attached to the wrist or ankle, has a broader use for\xa0**amputees**\xa0and other disabilities to allow people to regain the\xa0**joy of digital interaction once again**.\\r\\n\\r\\n---------------\\r\\n\\r\\n## \ud83e\udde0 What It Does\\r\\n\\r\\n**Gestura**\xa0enables users to\xa0**control their mouse using hand gestures**\xa0and perform mouse actions via\xa0**voice commands**.\\r\\n\\r\\n*   **Hand gestures**\xa0detected by the FREE-WILi move the mouse across the screen.\\r\\n*   **Voice commands**\xa0like:\\r\\n    *   \u201cleft click\u201d \\r\\n    *   \u201cright click\u201d\\r\\n    *   \u201cdouble click\u201d\\r\\n    *   \u201cscroll up\u201d\\r\\n    *   \u201cscroll down\u201d\\r\\n    *   \u201chold\u201d\\r\\n    *   \u201crelease\u201d\\r\\n    *   \u201cpause input\u201d\\r\\n    *   \u201cresume input\u201d\\r\\n        \\r\\n\\r\\nThis provides a\xa0**touch-free**, accessible experience for users with limited mobility.\\r\\n\\r\\n-------------------\\r\\n\\r\\n## \ud83d\udee0\ufe0f How We Built It\\r\\n\\r\\nWe used\xa0**Python**\xa0and developed collaboratively with\xa0**Git**\xa0and\xa0**GitHub**.\\r\\n\\r\\n### \ud83d\udce6 Libraries & Tools Used\\r\\n\\r\\n*   [`FREE-WILi`](https://pypi.org/project/freewili/): Interface with the FREE-WILi accelerometer.   \\r\\n*   `pynput`: Control the mouse (movement and clicks).\\r\\n*   `sounddevice`: Capture audio input via microphone.\\r\\n*   `vosk`: Offline voice recognition using\xa0vosk-model-small.\\r\\n    \\r\\n### \ud83c\udfae Mouse Movement via Accelerometer\\r\\n\\r\\n*   Captured\xa0**X**\xa0and\xa0**Y**\xa0acceleration using the\xa0FREE-WILi\xa0Python library. \\r\\n*   Applied\xa0**double integration**\xa0to convert acceleration to\xa0**position**.\\r\\n*   Used\xa0pynput.mouse.Controller.position\xa0to update the mouse location.\\r\\n\\r\\n### \ud83c\udf99 Voice Control\\r\\n\\r\\n*   Captured real-time audio using the\xa0**computer\u2019s microphone**.    \\r\\n*   Processed input through\xa0**Vosk**\xa0for speech recognition.\\r\\n*   Mapped voice inputs to actions like:\\r\\n    *   click\\r\\n    *   right click\\r\\n    *   scroll up/down\\r\\n    *   pause/resume input, etc.\\r\\n        \\r\\n-------------------\\r\\n\\r\\n## \ud83d\udea7 Challenges We Ran Into\\r\\n\\r\\n*   **Sensor Drift & Offset**: The raw accelerometer data wasn\u2019t zero at rest, causing significant drift after integration.\\r\\n    -  **Fix**: Introduced a\xa0**deadband**\xa0(e.g.,\xa0) to treat near-zero acceleration as zero.\\r\\n      \\r\\n*   **Erratic Movements**: Large spikes in acceleration led to uncontrollable motion.\\r\\n    -   **Fix**: Added\xa0**min/max bounds**\xa0for both velocity and acceleration to stabilize gestures.\\r\\n        \\r\\n*   **Screen Bounds**: The mouse pointer would sometimes go off-screen.\\r\\n    -   **Fix**: Implemented\xa0**position clamping**\xa0to ensure the mouse stays within the screen (1920x1080).\\r\\n        \\r\\n-------------------\\r\\n\\r\\n## \ud83c\udfc6 Accomplishments We\'re Proud Of\\r\\n\\r\\n*   **Natural-feeling gesture control**\xa0after some practice.\\r\\n*   **User feedback**: A friend testing it smiled when the device worked exactly as intended.\\r\\n*   Implemented a\xa0**broad command set**, from clicking to scrolling and pausing/resuming gesture input.\\r\\n    \\r\\n-------------------\\r\\n\\r\\n## \ud83d\udcda What We Learned\\r\\n\\r\\n*   The\xa0**FREE-WILi**\xa0device is powerful and versatile for prototyping assistive tech.\\r\\n*   Sensor data in the real world can be noisy\u2014handling that is both science and art.\\r\\n*   Combining\xa0**gesture + voice**\xa0control can dramatically improve accessibility.\\r\\n    \\r\\n--------------------------\\r\\n\\r\\n## \ud83d\ude80 What\u2019s Next for Gestura\\r\\n\\r\\n*   \ud83d\udee0\ufe0f Build a\xa0**gesture calibration tool**\xa0to personalize the experience.\\r\\n*   \ud83e\udde0 Add\xa0**machine learning**\xa0to adapt gesture detection to each user over time.\\r\\n*   \ud83c\udf10 Integrate with web browsers for scroll, tab switch, and navigation.\\r\\n*   \ud83c\udfa4 Use the\xa0**FREE-WILi\u2019s microphone**\xa0for full-device integration (once streaming support is added).\\r\\n\\r\\n--------------------------\\r\\n\\r\\n## \ud83e\udde9 Built With  \\r\\n\\r\\n```freewili```\\r\\n```git```\\r\\n```github```\\r\\n```pynput```\\r\\n```python```\\r\\n```sounddevice```\\r\\n```vosx```\\r\\n\\r\\n\\r\\n### Try it out\\r\\n\\r\\n<div class=\\"github-img\\">\\r\\n\\r\\n![GitHub](./github.png) <a href=\\"https://github.com/imzaynb/MHacks2025\\" target=\\"_blank\\"><span>GitHub Repo</span></a>\\r\\n</div>\\r\\n\\r\\n *Source* - [https://devpost.com/software/gestura-9oaugq/](https://devpost.com/software/gestura-9oaugq/)"},{"id":"celestaisle","metadata":{"permalink":"/blog/celestaisle","source":"@site/blog/2025-10-07/index.mdx","title":"celestaisle","description":"Optimize your shopping experience with celestaisle, an app linked to in-store pagers. Copy and paste your grocery list, select a supermarket, and pick up a compass to guide you through the store.","date":"2025-10-07T00:00:00.000Z","tags":[{"inline":false,"label":"FREE-WILi","permalink":"/blog/tags/FREE-WILi","description":"Make Embedded Systems Fun Again !"},{"inline":false,"label":"Mhacks","permalink":"/blog/tags/mhacks","description":"mhacks"},{"inline":false,"label":"Student Project","permalink":"/blog/tags/studentproject","description":"studentproject"},{"inline":false,"label":"Hackathon","permalink":"/blog/tags/hackathon","description":"hackathon"}],"readingTime":3.67,"hasTruncateMarker":true,"authors":[{"name":"Natalie Do","url":"https://devpost.com/donatali","imageURL":"https://lh3.googleusercontent.com/a/ACg8ocLlmOT8383I2gouoB-eIswvw62e-NJQm6jC_kiSrHdWAbmxaw=s96-c?height=180&width=180","key":"Natalie Do","page":null},{"name":"Abraham Vega","url":"https://devpost.com/abevega","imageURL":"https://lh3.googleusercontent.com/a/ACg8ocJDAkxy_8NpvHBs7HXX1umsCRBYdZ1RbBhHtHjpmDCPTrLPqA=s96-c?height=180&width=180","key":"Abraham Vega","page":null},{"name":"Ava Chang","url":"https://devpost.com/avachang","imageURL":"https://lh3.googleusercontent.com/a/ACg8ocKQn5QWGTA7-v2twdDycM5MMwVMYEfpM8FlurmBHvfizM-QTw=s96-c?height=180&width=180","key":"Ava Chang","page":null},{"name":"samira shalal","url":"https://devpost.com/n0vendy","imageURL":"https://avatars.githubusercontent.com/u/123274348?height=180&v=4&width=180.","key":"samira shalal","page":null}],"frontMatter":{"slug":"celestaisle","title":"celestaisle","authors":["Natalie Do","Abraham Vega","Ava Chang","samira shalal"],"date":"2025-10-07T00:00:00.000Z","tags":["FREE-WILi","mhacks","student project","hackathon"]},"unlisted":false,"prevItem":{"title":"\ud83d\udc4b Gestura: Gesture-Based Mouse & Voice Control","permalink":"/blog/gestura"},"nextItem":{"title":"Wattson - Nurture a pet by being sustainable","permalink":"/blog/wattson"}},"content":"> Optimize your shopping experience with celestaisle, an app linked to in-store pagers. Copy and paste your grocery list, select a supermarket, and pick up a compass to guide you through the store.\\r\\n\\r\\nimport MyCarousel from \'@site/src/components/Carousel\';\\r\\n\\r\\n{/* <MyCarousel /> */}\\r\\n\\r\\nexport const slides = [\\r\\n  { type: \'video\', src: \'mCyS_bO6Rtw\' }, // YouTube video\\r\\n  { type: \'image\', src: \'/img/blog/celestaisle-1.jpg\', alt: \'celestaisle-1\', caption: \\"Grocery items marked as nodes\\" },\\r\\n  { type: \'image\', src: \'/img/blog/celestaisle-2.jpg\', alt: \'celestaisle-2\', caption: \\"Live pathway\\" },\\r\\n  { type: \'image\', src: \'/img/blog/celestaisle-3.jpg\', alt: \'celestaisle-3\', caption: \\"Finished pathway\\" },\\r\\n  { type: \'image\', src: \'/img/blog/celestaisle-4.jpg\', alt: \'celestaisle-4\', caption: \\"FREE-WILi to FREE-WILi communication through IR\\" },\\r\\n];\\r\\n\\r\\n<MyCarousel slides={slides} />\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n<br/>\\r\\n\\r\\n## \u2728 Inspiration\\r\\n\\r\\nWe were inspired by another project in the same realm of grocery store experience improvement. Our name was inspired by the historical use of stars for navigation. Our goal is to benefit both the customer and employee, by allowing the customer to be more thoughtful about their choices and speeding up their grocery trip while streamlining customer service on the employee side.\\r\\n\\r\\n--------------------------\\r\\n\\r\\n## \ud83d\uded2 What it does\\r\\n\\r\\nCelestaisle is a smart shopping tool that streamlines shopping experiences from planning beforehand to in-store navigation. The app allows users to enter their zip code, provide a budget, and input their shopping list, which could include separate ingredients or potential recipes. The app looks for nearby stores and pulls item lists from each. Behind the scenes, AI is used to optimize and improve the shopping list, suggesting additional items the user may want and allowing misspellings. Options from different price ranges are given so that the user can choose between the generic brand, name brand, and organic options. Then, mapping information for a path through the store is sent to the FREE-WILi Compasses. Customers\' pagers (attached to their shopping carts) display the map to navigate through the stores and buttons can be pressed to indicate an item has been found. In addition, customer pagers can emit an IR signal to request help from an employee, turning on the LEDs. Each customer pager has its own color ID associated. When employee pagers receive that IR request, the customer is added to the help queue and the LEDs on the FREE-WILi pager display the queue by color ID. Once a customer is helped, employees can mark the customer as \\"helped\\", which removes their color ID from the LED queue, and the customer LED lights turn off through a received IR signal.\\r\\n\\r\\n--------------------------\\r\\n\\r\\n## \ud83d\udee0 How we built it\\r\\n\\r\\n**App & Backend**  \\r\\nWe used React Native and Expo for the app, with a python backend. The backend utilizes Flask and the Google Gemini API.\\r\\n\\r\\n**FREE-WILi Compasses**  \\r\\nWe programmed the FREE-WILi Compasses in Python using the provided interface documentation. We made use of the IR signals to communicate between FREE-WILi modules. We wirelessly uploaded custom images onto the screens and used the button and LED interface to allow customers to check off their path and page for help.\\r\\n\\r\\n--------------------------\\r\\n\\r\\n## \u26a0\ufe0f Challenges we ran into\\r\\n\\r\\nWe ran into big challenges with the FREE-WILi modules. We couldn\'t get CMake working at all, even with the help of the folks at FREE-WILi, so we ended up having to use the Python interface, which brought up some roadblocks with our initial implementation ideas, but we still managed to get the compasses working how we wanted. We were hoping to use RF to communicate long distance between the FREE-WILis, however, there ended up actually being an issue with the FREE-WILi API, so we were unable to use that feature.\\r\\n\\r\\n--------------------------\\r\\n\\r\\n## \ud83c\udfc6 Accomplishments that we\'re proud of\\r\\n\\r\\nWe are proud that we were able to get over the initial roadblock we faced with the CMake issues, and to have used an AI API in a project for the first time.\\r\\n\\r\\n--------------------------\\r\\n\\r\\n## \ud83d\udcda What we learned\\r\\n\\r\\nWe learned how using an AI API works, and how to take advantage of the FREE-WILi Device. Some members worked with frontend backend integration for the first time.\\r\\n\\r\\n--------------------------\\r\\n\\r\\n## \ud83d\udd2e What\'s next for Celestaisle\\r\\n\\r\\nWe would seek to use the FREE-WILi as a standalone device by uploading wasm files to the device.\\r\\n\\r\\n---\\r\\n\\r\\n## \ud83e\udde9 Built With  \\r\\n\\r\\n```expo.io```\\r\\n```gemini```\\r\\n```python```\\r\\n```reactnative```\\r\\n```sql```\\r\\n```typescript```\\r\\n\\r\\n\\r\\n### Try it out\\r\\n\\r\\n<div class=\\"github-img\\">\\r\\n\\r\\n![GitHub](./github.png) <a href=\\"https://github.com/n0vendy/celestaisle\\" target=\\"_blank\\"><span>GitHub Repo</span></a>\\r\\n</div>\\r\\n\\r\\n *Source* - [https://devpost.com/software/celestaisle/](https://devpost.com/software/celestaisle/)"},{"id":"wattson","metadata":{"permalink":"/blog/wattson","source":"@site/blog/2025-10-06/index.mdx","title":"Wattson - Nurture a pet by being sustainable","description":"{/  /}","date":"2025-10-06T00:00:00.000Z","tags":[{"inline":false,"label":"FREE-WILi","permalink":"/blog/tags/FREE-WILi","description":"Make Embedded Systems Fun Again !"},{"inline":false,"label":"Mhacks","permalink":"/blog/tags/mhacks","description":"mhacks"},{"inline":false,"label":"Student Project","permalink":"/blog/tags/studentproject","description":"studentproject"},{"inline":false,"label":"Hackathon","permalink":"/blog/tags/hackathon","description":"hackathon"}],"readingTime":3.84,"hasTruncateMarker":true,"authors":[{"name":"Ashton Ma","url":"https://devpost.com/ashma2583","imageURL":"https://avatars.githubusercontent.com/u/181252691","key":"Ashton Ma","page":null},{"name":"reedmanning","url":"https://devpost.com/reedmanning","imageURL":"https://avatars.githubusercontent.com/u/112449236","key":"reedmanning","page":null},{"name":"jvmueller Mueller","url":"https://devpost.com/jvmueller","imageURL":"https://avatars.githubusercontent.com/u/181908772","key":"jvmueller Mueller","page":null},{"name":"Yongha Cho","url":"https://devpost.com/chyongha","imageURL":"https://avatars.githubusercontent.com/u/113219692?height=180&v=4&width=180","key":"Yongha Cho","page":null}],"frontMatter":{"slug":"wattson","title":"Wattson - Nurture a pet by being sustainable","authors":["Ashton Ma","reedmanning","jvmueller Mueller","Yongha Cho"],"date":"2025-10-06T00:00:00.000Z","tags":["FREE-WILi","mhacks","student project","hackathon"]},"unlisted":false,"prevItem":{"title":"celestaisle","permalink":"/blog/celestaisle"},"nextItem":{"title":"Will I Study?","permalink":"/blog/will-i-study"}},"content":"import MyCarousel from \'@site/src/components/Carousel\';\\r\\n\\r\\n{/* <MyCarousel /> */}\\r\\n\\r\\nexport const slides = [\\r\\n  { type: \'video\', src: \'GzLRBSqCBSw\' }, // YouTube video\\r\\n  { type: \'image\', src: \'/img/blog/wattson-img1.jpg\', alt: \'wattson-img1\', caption: \\"\\" },\\r\\n  { type: \'image\', src: \'/img/blog/wattson-img2.jpg\', alt: \'wattson-img2\', caption: \\"\\" },\\r\\n];\\r\\n\\r\\n<MyCarousel slides={slides} />\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n<br/>\\r\\n\\r\\n## \ud83c\udf31 Inspiration\\r\\n\\r\\nIn our pursuit of a more sustainable society, we looked back on our average life when we were wastefully using electricity by leaving the lights on while outside. Beyond the obvious cost of an electricity bill, leaving the lights on costs around contributes to a larger, collective carbon footprint. To inspire a shift from unconscious waste to conscious conservation, we designed Wattson that tackles this challenge and empowers individuals to minimize society\u2019s carbon footprint.\\r\\n\\r\\n---\\r\\n\\r\\n## \ud83d\udca1 What it does\\r\\n\\r\\nWattson is aimed to reduce your electricity waste by encouraging users to minimize the time their light is on through gamified experiences. Wattson is a cute, virtual pet that you aim to keep alive and nurture. If you leave the lights on for too long, you risk losing your beloved Wattson. On the other hand, be eco-friendly and Wattson regains health and gives you points. Points are used in the shop to buy Wattson his favorite cosmetics, animations, and sounds. It all comes down to one simple mission: be eco-friendly and keep Wattson alive.\\r\\n\\r\\n---\\r\\n\\r\\n## \ud83e\udde0 How we built it\\r\\n\\r\\nWe developed the program through Python. The project is divided into three parts, the OpenCV model, backend environment and the user interface on the FREE-WILi.\\r\\n\\r\\nWe used a proprietary graphics innovation, built to scale from the ground up. Because of the lack of GUI for the python api of freeWili, and us only being able to push images to the screen, we utilized the pillow library to dynamically stitch images together to create a game ready gui, capable of running at just seconds per frame. With this robust solution, we can scale easy as the first ai powered lightbulb assistant.\\r\\n\\r\\nThe OpenCV model stores two image files (old_photo and new_photo). Every 15 seconds, old_photo is discarded, new_photo becomes old_photo, and a new photo is taken. The computer vision model calculates the difference in brightness between the old_photo and new_photo to detect if lights were switched on/off.\\r\\n\\r\\nThe backend environment mainly exists within the Pet class, where variables such as health, points, the amount of time the lights were on, etc. are tracked. Additionally, functions that control the conditions for losses and gains of a pet\'s health depending on the usage of lights are implemented. These functions and variables are utilized to connect with the user interface of the FREE-WILi device.\\r\\n\\r\\n--- \\r\\n\\r\\n## \ud83e\udde9 Challenges we ran into\\r\\n\\r\\nThe biggest challenge we encountered was with the FREE-WILi firmware. There was a huge learning curve with learning implementation with the Python API. \x3c!-- and there were functionalities that didn\u2019t work every time we wanted them to. --\x3e\\r\\n\\r\\n---\\r\\n\\r\\n## \ud83c\udfc6 Accomplishments that we\'re proud of\\r\\n\\r\\nWe are proud to say that we developed our own, custom graphics interface on FREE-WILi and a program that detects if the lights are switched on or off to interact with the self-designed pet. We believe this program will be a foundation for developing a greater program that tackles higher-stake environmental issues.\\r\\n\\r\\n---\\r\\n\\r\\n## \ud83c\udf93 What we learned\\r\\n\\r\\nWe learned about how difficult it is to develop a program that interacts with an embedded developmental tool. Developing a program solely on a computer and a program that interacts with multiple devices was another challenge. Sometimes, some problems are impossible to solve and we just need to find a work-around.\\r\\n\\r\\n---\\r\\n\\r\\n## \ud83d\ude80 What\'s next for Wattson\\r\\n\\r\\nOur next steps would be to broaden our usage in generative AI to effectively detect further scenarios where energy is being wasted. The generative AI will be used to add a personality on the pets to improve the interaction with the user by adding talking or sending texts to the user for reducing waste or turning off lights properly. Though we were not able to fully implement a system that tracks when the person leaves the room, we will further improve on the CV model to include such situations as well. Beside light, the project will expand to the detection for better sustainability, such as water, trash, food waste, and heat. Additionally, we will improve the user experience through adding interactions with other people by including leaderboards, clans, and battling (such as in pokemon).\\r\\n\\r\\n---\\r\\n\\r\\n## \ud83e\udde9 Built With  \\r\\n\\r\\n```freewili``` ```opencv``` ```python```\\r\\n\\r\\n\\r\\n\x3c!-- ### Try it out\\r\\n\\r\\n<div class=\\"github-img\\">\\r\\n\\r\\n![GitHub](./github.png) <a href=\\"https://github.com/Nailfighter/VisionCane\\" target=\\"_blank\\"><span>GitHub Repo</span></a>\\r\\n</div>\\r\\n\\r\\n *Source* - [https://devpost.com/software/visioncane/](https://devpost.com/software/visioncane/) --\x3e"},{"id":"will-i-study","metadata":{"permalink":"/blog/will-i-study","source":"@site/blog/2025-10-06/will-i-study.mdx","title":"Will I Study?","description":"Keep yourself accountable with a study session tracker! Learn about your study habits and improve over time.","date":"2025-10-06T00:00:00.000Z","tags":[{"inline":false,"label":"FREE-WILi","permalink":"/blog/tags/FREE-WILi","description":"Make Embedded Systems Fun Again !"},{"inline":false,"label":"Mhacks","permalink":"/blog/tags/mhacks","description":"mhacks"},{"inline":false,"label":"Student Project","permalink":"/blog/tags/studentproject","description":"studentproject"},{"inline":false,"label":"Hackathon","permalink":"/blog/tags/hackathon","description":"hackathon"}],"readingTime":2.13,"hasTruncateMarker":true,"authors":[{"name":"XZCendence","url":"https://devpost.com/XZCendence","imageURL":"https://avatars.githubusercontent.com/u/44044176?height=180&v=4&width=180","key":"XZCendence","page":null},{"name":"Alexia Moreno","url":"https://devpost.com/alexiamoreno","imageURL":"https://avatars.githubusercontent.com/u/143415481?height=180&v=4&width=180","key":"Alexia Moreno","page":null},{"name":"nthaitang7","url":"https://devpost.com/nthaitang7","imageURL":"https://avatars.githubusercontent.com/u/110940160?height=180&v=4&width=180","key":"nthaitang7","page":null},{"name":"Nia Marzouca","url":"https://devpost.com/niaz666","imageURL":"https://lh3.googleusercontent.com/a/ACg8ocLsMwxVi54T8nV2EgUmkhgW4rdntZ3SZqjET8F_9E3aeunFbw=s96-c?height=180&width=180","key":"Nia Marzouca","page":null}],"frontMatter":{"slug":"will-i-study","title":"Will I Study?","authors":["XZCendence","Alexia Moreno","nthaitang7","Nia Marzouca"],"date":"2025-10-06T00:00:00.000Z","tags":["FREE-WILi","mhacks","student project","hackathon"]},"unlisted":false,"prevItem":{"title":"Wattson - Nurture a pet by being sustainable","permalink":"/blog/wattson"},"nextItem":{"title":"VisionCane - Empowering independence through intelligent navigation","permalink":"/blog/visioncane"}},"content":"> Keep yourself accountable with a study session tracker! Learn about your study habits and improve over time.\\r\\n\\r\\nimport MyCarousel from \'@site/src/components/Carousel\';\\r\\n\\r\\n{/* <MyCarousel /> */}\\r\\n\\r\\nexport const slides = [\\r\\n  { type: \'video\', src: \'BtIwf_Kr7ZY\' }, // YouTube video\\r\\n  { type: \'image\', src: \'/img/blog/will-i-study-img1.jpg\', alt: \'will-i-study-img1\', caption: \\"Dashboard shows various metrics relating to the user\'s focus level and environment\\" },\\r\\n  { type: \'image\', src: \'/img/blog/will-i-study-img2.jpg\', alt: \'will-i-study-img2\', caption: \\"Dashboard turns red when user is visibly distracted for a while\\" },\\r\\n];\\r\\n\\r\\n<MyCarousel slides={slides} />\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n<br/>\\r\\n\\r\\n## \ud83c\udf1f Inspiration\\r\\nWe were inspired by noticing what had worked well in our own study sessions, which was always a form of tracking, and taking it to the next level with this project.\\r\\n\\r\\n--- \\r\\n\\r\\n## \ud83d\udee0\ufe0f What it does\\r\\nIt tracks your study sessions using computer vision to make sure you stay on task and record analytics of how focused you are throughout your session. It gives recommendations based on how your study sessions went.\\r\\n\\r\\n---\\r\\n\\r\\n## \ud83d\udcbb How we built it\\r\\nWe have the front-end built in React letting us put together a very visually appealing dynamic website, we wrote the back-end in Go for flexibility and performance, and we interfaced with the hardware using Python for its versatility. Using different tooling was a good opportunity for each of us to bring our own expertise to the table and to learn from each other.\\r\\n\\r\\n---\\r\\n\\r\\n## \u26a0\ufe0f Challenges we ran into\\r\\nWorking with hardware is always very finicky, however we managed to reliably get the camera and microphone working as data points that our code could use.\\r\\n\\r\\n---\\r\\n\\r\\n## \ud83c\udfc6 Accomplishments that we\'re proud of\\r\\nWe are proud of our session management system so we can compactly store previous session data in order to compare your recent study sessions against previous ones, allowing for further insight.\\r\\n\\r\\n---\\r\\n\\r\\n## \ud83d\udcda What we learned\\r\\nWe learnt a lot about how to integrate embedded systems into a web dashboard in a unified way with the back-end.\\r\\n\\r\\n---\\r\\n\\r\\n## \ud83d\ude80 What\'s next for Will I Study?\\r\\nThe next steps would be to take more advantage of the hardware, as the more useful data points we get the better predictions and recommendations we can give. There are plenty of options, but integrating with screen capture or eye tracking would be a good choice.\\r\\n\\r\\n---\\r\\n\\r\\n## \ud83e\udde9 Built With  \\r\\n\\r\\n```cloudflare``` ```echo``` ```embedder``` ```freewili```\\r\\n```go```\\r\\n```javascript```\\r\\n```python```\\r\\n```react```\\r\\n```recharts```\\r\\n```shadcn```\\r\\n```vite```\\r\\n\\r\\n### Try it out\\r\\n\\r\\n<div class=\\"github-img\\">\\r\\n\\r\\n![GitHub](./github.png) <a href=\\"https://github.com/XZCendence/labubu25\\" target=\\"_blank\\"><span>GitHub Repo</span></a>\\r\\n</div>\\r\\n\\r\\n *Source* - [https://devpost.com/software/will-i-study](https://devpost.com/software/will-i-study)"},{"id":"visioncane","metadata":{"permalink":"/blog/visioncane","source":"@site/blog/2025-10-03/index.mdx","title":"VisionCane - Empowering independence through intelligent navigation","description":"\u2728 VisionCane is a step towards empowering visually impaired individuals with affordable, intelligent assistive technology.","date":"2025-10-03T00:00:00.000Z","tags":[{"inline":false,"label":"FREE-WILi","permalink":"/blog/tags/FREE-WILi","description":"Make Embedded Systems Fun Again !"},{"inline":false,"label":"Mhacks","permalink":"/blog/tags/mhacks","description":"mhacks"},{"inline":false,"label":"Student Project","permalink":"/blog/tags/studentproject","description":"studentproject"},{"inline":false,"label":"Hackathon","permalink":"/blog/tags/hackathon","description":"hackathon"}],"readingTime":2.47,"hasTruncateMarker":true,"authors":[{"name":"Shreyansh Sahu","url":"https://devpost.com/alphagameryt024","imageURL":"https://d112y698adiu2z.cloudfront.net/photos/production/user_photos/003/790/683/datas/profile.jpg","key":"Shreyansh Sahu","page":null}],"frontMatter":{"slug":"visioncane","title":"VisionCane - Empowering independence through intelligent navigation","authors":["Shreyansh Sahu"],"tags":["FREE-WILi","mhacks","student project","hackathon"]},"unlisted":false,"prevItem":{"title":"Will I Study?","permalink":"/blog/will-i-study"},"nextItem":{"title":"Wili pass","permalink":"/blog/wili-pass"}},"content":"> \u2728 VisionCane is a step towards empowering visually impaired individuals with affordable, intelligent assistive technology.\\r\\n\\r\\nimport MyCarousel from \'@site/src/components/Carousel\';\\r\\n\\r\\n{/* <MyCarousel /> */}\\r\\n\\r\\nexport const slides = [\\r\\n  { type: \'image\', src: \'/img/blog/visioncane.jpg\', alt: \'visioncane\', caption: \\"visioncane\\" },\\r\\n  { type: \'image\', src: \'/img/blog/stick.jpg\', alt: \'stick\', caption: \\"\\" },\\r\\n  { type: \'image\', src: \'/img/blog/stick-freewili.jpg\', alt: \'stick-freewili\', caption: \\"\\" },\\r\\n];\\r\\n\\r\\n<MyCarousel slides={slides} />\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n<br/>\\r\\n\\r\\n## \ud83c\udf1f Inspiration\\r\\n\\r\\nI was inspired by the daily challenges faced by visually impaired individuals in navigating unfamiliar environments and wanted to create an affordable, accessible solution that could enhance their independence and safety using modern technology.\\r\\n\\r\\n---\\r\\n\\r\\n## \ud83d\udc40 What it does\\r\\n\\r\\nVisionCane uses YOLO object detection to identify obstacles and landmarks, HC-SR04 ultrasonic sensors for precise distance measurement, and FREE-WILi\'s LIS3DH accelerometer to detect cane movement and orientation. The integrated speaker provides real-time audio feedback with spatial cues to help users navigate safely through their surroundings.\\r\\n\\r\\n---\\r\\n\\r\\n## \ud83d\udee0\ufe0f How I built it\\r\\n\\r\\nI\'m building a modular architecture with separate components: `CameraModule.py` for camera initialization, `objectDetection.py` for YOLO-based obstacle detection, `ultraSonic.py` for distance sensing, `stateMachine.py` for decision logic, and `main.py` as the central controller. The system integrates with FREE-WILi\'s API to access the LIS3DH accelerometer for motion detection and the digital speaker for audio feedback, processing real-time sensor fusion to generate intelligent navigation assistance. \\r\\n\\r\\n---\\r\\n\\r\\n## \u2694\ufe0f Challenges I ran into\\r\\n\\r\\nIntegrating real-time computer vision processing with ultrasonic sensor data and FREE-WILi accelerometer readings while maintaining low latency, ensuring reliable object detection across different lighting conditions, synchronizing multiple sensor inputs through the FREE-WILi API, and creating an intuitive audio feedback system that provides useful spatial information without overwhelming users.\\r\\n\\r\\n---\\r\\n\\r\\n## \ud83c\udfc6 Accomplishments I\u2019m proud of\\r\\n\\r\\nSuccessfully setting up the modular architecture with separate components for camera handling, object detection, ultrasonic sensing, and state management. Implementing integration with FREE-WILi\'s LIS3DH accelerometer for motion detection and speaker for audio feedback, creating a clean separation of concerns that allows for easy testing and debugging of individual components.\\r\\n\\r\\n---\\r\\n\\r\\n## \ud83d\udcda What I learned\\r\\n\\r\\nThe importance of modular design in complex sensor fusion projects, how to integrate FREE-WILi\'s API for accelerometer data streaming and speaker control, structuring a state machine for real-time decision making with multiple sensor inputs, the challenges of synchronizing computer vision, ultrasonic, and accelerometer data, and the critical need for user-centered design when creating assistive technology that must be reliable and intuitive.\\r\\n\\r\\n---\\r\\n\\r\\n## \ud83d\ude80 What\u2019s next for VisionCane  \\r\\n\\r\\nCompleting the implementation of each module, integrating the YOLOv8 model for object detection, implementing FREE-WILi accelerometer event handling for motion-based state changes, adding spatial audio feedback generation through the FREE-WILi speaker, implementing the state machine logic for navigation decisions, and testing the complete system with real-world scenarios to refine the user experience and optimize sensor fusion algorithms.\\r\\n\\r\\n---\\r\\n\\r\\n## \ud83e\udde9 Built With  \\r\\n\\r\\n```embedded``` ```freewili``` ```python``` ```raspberry-pi```\\r\\n\\r\\n---\\r\\n\\r\\n### Try it out\\r\\n\\r\\n<div class=\\"github-img\\">\\r\\n\\r\\n![GitHub](./github.png) <a href=\\"https://github.com/Nailfighter/VisionCane\\" target=\\"_blank\\"><span>GitHub Repo</span></a>\\r\\n</div>\\r\\n\\r\\n*Source* - [https://devpost.com/software/visioncane/](https://devpost.com/software/visioncane/)"},{"id":"wili-pass","metadata":{"permalink":"/blog/wili-pass","source":"@site/blog/2025-05-22/index.mdx","title":"Wili pass","description":"Nintendo DS Street Pass, where passing strangers unlocked virtual connections. We\'re bringing that experience into the hacker community with Wili Pass. Share your hacking profile with passing hackers.","date":"2025-05-22T00:00:00.000Z","tags":[{"inline":false,"label":"FREE-WILi","permalink":"/blog/tags/FREE-WILi","description":"Make Embedded Systems Fun Again !"},{"inline":false,"label":"GrizzHacks 7","permalink":"/blog/tags/grizzhacks7","description":"grizzhacks7"},{"inline":false,"label":"Student Project","permalink":"/blog/tags/studentproject","description":"studentproject"},{"inline":false,"label":"Hackathon","permalink":"/blog/tags/hackathon","description":"hackathon"}],"readingTime":1.58,"hasTruncateMarker":true,"authors":[{"name":"Joey Volcic","url":"https://devpost.com/Joeyvolcic","imageURL":"https://avatars.githubusercontent.com/u/36684335?v=4","key":"Joey Volcic","page":null},{"name":"chomayouni Homayouni","url":"https://devpost.com/chomayouni","imageURL":"https://avatars.githubusercontent.com/u/69778851?v=4","key":"chomayouni Homayouni","page":null}],"frontMatter":{"slug":"wili-pass","title":"Wili pass","authors":["Joey Volcic","chomayouni Homayouni"],"tags":["FREE-WILi","GrizzHacks 7","student project","hackathon"]},"unlisted":false,"prevItem":{"title":"VisionCane - Empowering independence through intelligent navigation","permalink":"/blog/visioncane"},"nextItem":{"title":"Project Story: Safe X Guardian","permalink":"/blog/safe-x-guardian"}},"content":"> _Nintendo DS Street Pass, where passing strangers unlocked virtual connections. We\'re bringing that experience into the hacker community with Wili Pass. Share your hacking profile with passing hackers._\\r\\n\\r\\nimport MyCarousel from \'@site/src/components/Carousel\';\\r\\n\\r\\n{/* <MyCarousel /> */}\\r\\n\\r\\nexport const slides = [\\r\\n  { type: \'video\', src: \'CteTgYjheqw\' }, // YouTube video\\r\\n  { type: \'image\', src: \'/img/blog/grizzhacks7-gallery.jpg\', alt: \'grizzhacks7\', caption: \\"Connor\'s Hacking Passport\\" },\\r\\n  { type: \'image\', src: \'/img/blog/grizzhacks7-gallery2.jpg\', alt: \'grizzhacks7_project\', caption: \\"Joey\'s Hacking Passport\\" },\\r\\n  \\r\\n];\\r\\n\\r\\n<MyCarousel slides={slides} />\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n<br/>\\r\\n\\r\\n## \ud83c\udf1f Inspiration\\r\\nWe were inspired by the nostalgic charm of Nintendo DS StreetPass, where chance encounters led to exciting virtual interactions. We wanted to bring that sense of discovery and connection to the hacker community, fostering real-world interactions in a digital age.\\r\\n\\r\\n<hr/>\\r\\n\\r\\n## \ud83c\udfb5 What it does\\r\\nWili-Pass allows hackers to exchange digital \'passports\' when their FREE-WILi devices come within range. These passports contain user-defined profiles, including stats, personalized images, and audio messages. It\'s a way to instantly learn about and connect with fellow hackers at events and gatherings.\\r\\n\\r\\n<hr/> \\r\\n\\r\\n## \ud83d\udee0\ufe0f How we built it\\r\\nWe used the FREE-WILi dev board\'s antenna capabilities to detect near by hackers. Once detected, the unique hacker signature is used to query their profile. The passing hacker profile is then displayed for each hacker to view and connect with one another.\\r\\n\\r\\n<hr/>\\r\\n\\r\\n## \ud83d\udea7 Challenges we ran into\\r\\nCommunicating with the FREE-WILi over antenna proved to be very challenging. Getting the environment and tool chain setup, was also harder than expected.\\r\\n\\r\\n<hr/>\\r\\n\\r\\n## \ud83c\udfc6 Accomplishments that we\'re proud of\\r\\nCommunicating between boards.\\r\\n\\r\\n<hr/>\\r\\n\\r\\n## \ud83d\udcda What we learned\\r\\nPlanning and re-evaluating feasibility of ideas.\\r\\n\\r\\n<hr/>\\r\\n\\r\\n## \ud83d\ude80 What\'s next for Wili pass\\r\\nWide spread hackathon use!\\r\\n\\r\\n<hr/>\\r\\n\\r\\n### Built With\\r\\n\\r\\n```c++``` ```freewili```\\r\\n\\r\\n### Try it out\\r\\n\\r\\n<div class=\\"github-img\\">\\r\\n\\r\\n![GitHub](./github.png) <a href=\\"https://github.com/Joeyvolcic/G-RIZZ-HACK\\" target=\\"_blank\\"><span>GitHub Repo</span></a>\\r\\n</div>\\r\\n\\r\\n*Source* - [https://devpost.com/software/wili-pass](https://devpost.com/software/wili-pass)"},{"id":"safe-x-guardian","metadata":{"permalink":"/blog/safe-x-guardian","source":"@site/blog/2025-02-21/index.mdx","title":"Project Story: Safe X Guardian","description":"];","date":"2025-02-21T00:00:00.000Z","tags":[{"inline":false,"label":"FREE-WILi","permalink":"/blog/tags/FREE-WILi","description":"Make Embedded Systems Fun Again !"},{"inline":false,"label":"SpartaHack X","permalink":"/blog/tags/spartahack-x","description":"spartahack-x"},{"inline":false,"label":"Hackathon","permalink":"/blog/tags/hackathon","description":"hackathon"},{"inline":false,"label":"Student Project","permalink":"/blog/tags/studentproject","description":"studentproject"}],"readingTime":2.37,"hasTruncateMarker":true,"authors":[{"name":"Suryaprakash Veeramani","title":"CS Student @ Michigan State University","url":"https://www.linkedin.com/in/suryaprakash-veeramani-87304b21b","imageURL":"https://static.licdn.com/aero-v1/sc/h/9c8pery4andzj6ohjkjp54ma2","key":"Suryaprakash Veeramani","page":null},{"name":"Saurav Wadhwa","title":"CS Student @ Michigan State University","url":"https://devpost.com/wadhwasa","imageURL":"https://static.licdn.com/aero-v1/sc/h/9c8pery4andzj6ohjkjp54ma2","key":"Saurav Wadhwa","page":null}],"frontMatter":{"slug":"safe-x-guardian","title":"Project Story: Safe X Guardian","authors":["Suryaprakash Veeramani","Saurav Wadhwa"],"tags":["FREE-WILi","SpartaHack X","hackathon","student project"]},"unlisted":false,"prevItem":{"title":"Wili pass","permalink":"/blog/wili-pass"},"nextItem":{"title":"thereMINI: Creating Music with Hand Gestures Using FREE-WILi for a Touch-Free Sound Experience","permalink":"/blog/theremini"}},"content":"import MyCarousel from \'@site/src/components/Carousel\';\\r\\n\\r\\nexport const slides = [\\r\\n  { type: \'image\', src: \'/img/blog/raspb.png\', alt: \'raspb\', caption: \'Raspberry Pi with camera connected\' },\\r\\n  { type: \'image\', src: \'/img/blog/freewili-machine.png\', alt: \'freewilimachine\', caption: \'Freewilli machine connected with 3 antennas for communication, tracking with microphone, motion sensor, etc\'},\\r\\n  { type: \'image\', src: \'/img/blog/fw-wb.jpg\', alt: \'fwwb\', caption: \'Free Willi Wristband that gets sent notifications of motion detection, camera detection, and much more.\' },\\r\\n\\r\\n];\\r\\n\\r\\n<MyCarousel slides={slides} />\\r\\n\\r\\n<br/>\\r\\n\\r\\n**About the Project** The inspiration for Safe X Guardian came from the need to enhance security around personal vehicles and property. In a world where thefts and break-ins are a constant concern, I wanted to create a system that could actively monitor and protect my home and vehicle from unauthorized access. \x3c!-- truncate --\x3e Starting with basic components like motion sensors and facial recognition, I gradually built upon this concept to integrate features such as voice recognition and night vision for 24/7 security.\\r\\n\\r\\n**What I Learned** Throughout the process, I learned how to integrate AI-powered systems with hardware like PIR motion sensors, cameras, and microphones. I explored facial recognition and voice detection technologies, gaining experience with Raspberry Pi and other sensors to improve security features. This project expanded my knowledge of system integration, real-time monitoring, and how to handle different types of inputs to create a cohesive, functional security system.\\r\\n\\r\\n**Building the Project** I began with the idea of a simple security system using motion detection and a basic camera for monitoring. As I progressed, I added PIR motion sensors to detect movement around the vehicle and property, along with facial recognition to identify individuals. The integration of voice recognition and night vision capabilities brought the system to a new level, enabling it to work effectively even in low-light environments.\\r\\n\\r\\n**Challenges Faced** The main challenges included configuring the facial recognition software and ensuring it could detect faces accurately from various angles and distances. Integrating all the components, including PIR sensors, camera systems, and voice recognition, required troubleshooting and optimizing the connections for smooth real-time performance. Another challenge was calibrating the motion detection system to avoid false alerts, which took time to fine-tune.\\r\\n\\r\\n**From Start to Finish** What I started with was a basic concept of monitoring using motion sensors, evolving it into a fully integrated vehicle and property monitoring system that can detect motion, identify individuals, and provide alerts in real-time. The final project, Safe X Guardian, is a smart, comprehensive security solution that actively monitors surroundings, providing enhanced protection for both vehicles and property.\\r\\n\\r\\n### Built With\\r\\n\\r\\n```c``` ```freewili``` ```python``` ```raspberry-pi``` ```whadda```\\r\\n\\r\\n### Try it out\\r\\n\\r\\n<div class=\\"github-img\\">\\r\\n\\r\\n![GitHub](./github.png) <a href=\\"https://github.com/Akshith-AI/Spartahack-X\\" target=\\"_blank\\"><span>GitHub Repo</span></a>\\r\\n</div>\\r\\n\\r\\n*Source* - [https://devpost.com/software/safe-x-guardian](https://devpost.com/software/safe-x-guardian)"},{"id":"theremini","metadata":{"permalink":"/blog/theremini","source":"@site/blog/2025-02-06/index.mdx","title":"thereMINI: Creating Music with Hand Gestures Using FREE-WILi for a Touch-Free Sound Experience","description":"The Wearable Theremin! thereMINI is a wearable theremin using FREE-WILi to turn hand movements into music, translating motion into notes and volume for a touch-free, expressive sound experience.","date":"2025-02-06T00:00:00.000Z","tags":[{"inline":false,"label":"FREE-WILi","permalink":"/blog/tags/FREE-WILi","description":"Make Embedded Systems Fun Again !"},{"inline":false,"label":"SpartaHack X","permalink":"/blog/tags/spartahack-x","description":"spartahack-x"},{"inline":false,"label":"Hackathon","permalink":"/blog/tags/hackathon","description":"hackathon"},{"inline":false,"label":"Student Project","permalink":"/blog/tags/studentproject","description":"studentproject"}],"readingTime":3.6,"hasTruncateMarker":true,"authors":[{"name":"Evan Fioritto","title":"Michigan State University Junior Majoring in Computer Science, Minoring in Information Technology","url":"https://www.linkedin.com/in/evanfioritto/","imageURL":"https://media.licdn.com/dms/image/v2/D5603AQGnUgiBv-EyUQ/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1725555942934?e=2147483647&v=beta&t=EZGdis3uVdg-VTrpbg5lFxDJ3zfN2TJWS2KDleEK1Dk","key":"Evan Fioritto (fiorittoev)","page":null},{"name":"Richard Glosser","title":"Computer Science @ Michigan State University","url":"https://www.linkedin.com/in/richard-glosser-76145426b","imageURL":"https://d112y698adiu2z.cloudfront.net/photos/production/user_photos/003/042/349/datas/profile.jpg","key":"Richard Glosser","page":null},{"name":"Tess Gonda","title":"CSE Student @ Michigan State University","url":"https://www.linkedin.com/in/tess-gonda-abb1b5309","imageURL":"https://media.licdn.com/dms/image/v2/D4D03AQFEjwMKg6LocQ/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1715288605164?e=2147483647&v=beta&t=MYl5stpGSNIVDulr1GvNM464u2yySSsfh0QWm4AIatw","key":"Tess Gonda","page":null},{"name":"Ramya Ramkumar","title":"CS Student @ Michigan State University","url":"https://www.linkedin.com/in/ramyaramkumar","imageURL":"https://media.licdn.com/dms/image/v2/D5603AQHSYB6UnKWDNw/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1718283609856?e=2147483647&v=beta&t=P6G5ZGE6kc8C50y8oU5YDiAKRkPlgK-FV8eRhuu6wuA","key":"Ramya Ramkumar","page":null}],"frontMatter":{"slug":"theremini","title":"thereMINI: Creating Music with Hand Gestures Using FREE-WILi for a Touch-Free Sound Experience","authors":["Evan Fioritto (fiorittoev)","Richard Glosser","Tess Gonda","Ramya Ramkumar"],"tags":["FREE-WILi","SpartaHack X","hackathon","student project"]},"unlisted":false,"prevItem":{"title":"Project Story: Safe X Guardian","permalink":"/blog/safe-x-guardian"},"nextItem":{"title":"WiLi-Party LED Minigames","permalink":"/blog/wiLi-party"}},"content":"The Wearable Theremin! thereMINI is a wearable theremin using FREE-WILi to turn hand movements into music, translating motion into notes and volume for a touch-free, expressive sound experience.\\r\\n\\r\\nimport MyCarousel from \'@site/src/components/Carousel\';\\r\\n\\r\\n{/* <MyCarousel /> */}\\r\\n\\r\\nexport const slides = [\\r\\n  { type: \'video\', src: \'W0wFaAqFdok\' }, // YouTube video\\r\\n  { type: \'image\', src: \'/img/blog/logo.jpg\', alt: \'logo\', caption: \'Our Logo and Mascot, T-Mini! (cr. minipete)\' },\\r\\n  { type: \'image\', src: \'/img/blog/gallery.jpg\', alt: \'gallery\', caption: \\"How it all works (its magic, I really don\'t know how we got it to work)\\" },\\r\\n  { type: \'image\', src: \'/img/blog/gallery-freewili.jpg\', alt: \'Gallery-FREE-WILi\', caption: \'the child witnessing its birth\' },\\r\\n  { type: \'image\', src: \'/img/blog/gallery-1.png\', alt: \'Gallery-FREE-WILi-1\', caption: \'my wrist\'},\\r\\n  { type: \'image\', src: \'/img/blog/gui.png\', alt: \'gui\', caption: \'gui\' },\\r\\n  { type: \'image\', src: \'/img/blog/code.png\', alt: \'code\', caption: \'painful amounts of painful code\'},\\r\\n  \\r\\n];\\r\\n\\r\\n<MyCarousel slides={slides} />\\r\\n\\r\\n\x3c!-- truncate --\x3e\\r\\n\\r\\n## \ud83c\udfb6 thereMINI: A Wearable Theremin \ud83c\udfb6 ##\\r\\n\\r\\n*Turning motion into music with FREE-WILi*\\r\\n\\r\\n<hr/>\\r\\n\\r\\n## \ud83c\udf1f Inspiration\\r\\n\\r\\nWe designed this as an accessibility device for one-handed music creation but soon saw its broader potential. Inspired by conducting, we mapped hand motion to notes and volume, making music creation more intuitive and inclusive. We continue to design this as a accessible device, but it is for everyone!\\r\\n\\r\\n<hr/>\\r\\n\\r\\n## \ud83c\udfb5 What It Does\\r\\nUsing the FREE-WILi we were able to extract the accelerometer data as serial data. Converting said data to MIDI data (a music data type) we were able to use music software, like Ableton to translate it into virtual instrument playing. Moving your hand down increases volume and putting it down increases it. Moving your hand left or right changes notes, specifically a scale of 8 notes. Combining all that we can emulate a full virtual keyboard with just the movement of your hand!\\r\\n\\r\\n<hr/>\\r\\n\\r\\n## \ud83d\udee0\ufe0f How We Built It\\r\\n\\r\\n* **Hardware:** FREE-WILi device is a wearable microprocessor, outfitted with many sensors and capabilities, the one we focused on is the accelerometer. We used this mainly to translate xyz coordinates into angles, and then MIDI values for the notes and volume. This data is sent through USB serial data to be read by the software.\\r\\n* **Software:** We used C++, and C to embed functionality for the FREE-WILi and python to clean and extract the data from it. loopMIDI is a software that creates a virtual MIDI controller that reads from our script, and feeds it to DAWs.\\r\\n* **Implementation:** The horizontal rotation is used for the notes on a rotation of -90 -> 90 degrees, and the vertical rotation is used for lowering and increasing the pitch on -30 -> 30 degrees.\\r\\n\\r\\n<hr/>\\r\\n\\r\\n## \ud83d\udea7 Challenges We Ran Into\\r\\nWe addressed USB port access challenges between WSL and Windows, ensuring proper connectivity. Translating accelerometer data into MIDI required refining sensitivity and applying filtering to improve accuracy. By optimizing data formatting and transmission, we facilitated seamless MIDI transfer. Through continuous troubleshooting and iterative testing, we achieved smooth integration between hardware and software.\\r\\n\\r\\n<hr/>\\r\\n\\r\\n## \ud83c\udfc6 Accomplishments We\'re Proud Of\\r\\nOur project focused on turning complex tasks into key milestones. A major achievement was enabling MIDI data transfer from FREE-WILi to a computer, converting user movements into precise musical instructions like pitch and velocity. Our greatest achievement was the design of the notes and pitch to hand movements, and what was the most ergonomic and accessible way we would implement it.\\r\\n\\r\\n<hr/>\\r\\n\\r\\n## \ud83d\udcda What We Learned\\r\\nWe learned how to process real-time sensor data into MIDI signals, ensuring seamless hardware-software integration. Designing for accessibility taught us the importance of intuitive and ergonomic control. Optimizing MIDI implementation required refining pitch, velocity, and minimizing latency. Through iterative problem-solving, we debugged formatting errors and fine-tuned movement sensitivity.\\r\\n\\r\\n<hr/>\\r\\n\\r\\n## \ud83d\ude80 What\'s Next for thereMINI\\r\\nWe plan to enhance thereMINI with scale mode, chord mode, and octave control, mapped to buttons on the FREE-WILi for seamless switching. A GUI will provide visual feedback and customization options. Beyond development, thereMINI has potential as an accessible instrument, an innovative tool for musicians, and an educational device for learning music through motion.\\r\\n\\r\\n<hr/>\\r\\n\\r\\n### Built With\\r\\n\\r\\n```ableton``` ```accelerometer``` ```c``` ```c++``` ```freewili``` ```loopmidi``` ```pyserial``` ```python``` ```rtmidi```\\r\\n\\r\\n### Try it out\\r\\n\\r\\n<div class=\\"github-img\\">\\r\\n\\r\\n![GitHub](./github.png) <a href=\\"https://github.com/fiorittoev/theremini\\" target=\\"_blank\\"><span>GitHub Repo</span></a>\\r\\n</div>\\r\\n\\r\\n*Source* - [https://devpost.com/software/theremini](https://devpost.com/software/theremini)"},{"id":"wiLi-party","metadata":{"permalink":"/blog/wiLi-party","source":"@site/blog/2024-12-10/index.md","title":"WiLi-Party LED Minigames","description":"Inspiration","date":"2024-12-10T00:00:00.000Z","tags":[{"inline":false,"label":"FREE-WILi","permalink":"/blog/tags/FREE-WILi","description":"Make Embedded Systems Fun Again !"},{"inline":false,"label":"Mhacks","permalink":"/blog/tags/mhacks","description":"mhacks"},{"inline":false,"label":"Hackathon","permalink":"/blog/tags/hackathon","description":"hackathon"},{"inline":false,"label":"Student Project","permalink":"/blog/tags/studentproject","description":"studentproject"}],"readingTime":2.38,"hasTruncateMarker":true,"authors":[{"name":"Andi Shaska","title":"CSE @ University of Michigan","url":"https://www.linkedin.com/in/andi-shaska","imageURL":"https://media.licdn.com/dms/image/v2/D5603AQHXrZrh8KFeEw/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1703796143917?e=2147483647&v=beta&t=RqiGYItuNXyk1k7RFprjvBrMkEkYIgivHqAJpiA8XCg","key":"Andi Shaska","page":null},{"name":"Harshad Bhojan","title":"Computer Science @ University of Michigan CoE","url":"https://www.linkedin.com/in/harshad-bhojan","imageURL":"https://media.licdn.com/dms/image/v2/C4E03AQEIRmbQhhAB5w/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1608520744606?e=2147483647&v=beta&t=g5S5Fgc41al3WUtTO6aVXK_vjtp1L2QcMQTgVorbTzg","key":"Harshad Bhojan","page":null}],"frontMatter":{"slug":"wiLi-party","title":"WiLi-Party LED Minigames","authors":["Andi Shaska","Harshad Bhojan"],"date":"2024-12-10T00:00:00.000Z","tags":["FREE-WILi","mhacks","hackathon","student project"]},"unlisted":false,"prevItem":{"title":"thereMINI: Creating Music with Hand Gestures Using FREE-WILi for a Touch-Free Sound Experience","permalink":"/blog/theremini"},"nextItem":{"title":"WiLi Watch \u2013 Voice-Activated Wearable for Smart Home Control","permalink":"/blog/wiLi-watch"}},"content":"import YouTubeEmbed from \'@site/src/components/YouTubeEmbed\';\\n\\n<YouTubeEmbed videoId=\\"7VzAm0KVVoo\\" />\\n\\n<br/>\\n\\n## Inspiration\\n\\nWe love competitive games like Mario Party and the cuteness of small devices like Tamagotchi. We wanted a project that combines the two, which is why the free wili seemed like a great system to use.\\n\\n\x3c!-- truncate --\x3e\\n\\n## What it does\\n\\nGives users the ability to play four 1v1 games\\n\\n1) Mashing game - Who can rack up the highest number of button presses before time runs out?\\n2) Memory game - A pattern sequence, up to 3 levels of increasing difficulty. Test your memory!\\n3) Counting game - Keep track of your color light and how many times it flashes. Don\'t blink.\\n4) Reaction game - Who can react faster? Click the color that shows up as quick as you can.\\n\\nThe lights at the end of a game blink red if P1 has won and green if P2 (yellow if tie).\\n\\n## How we built it\\n\\nWe used two Python libraries. The FreeWheel API to communicate with the FREE-WILi and flash our code, and the pygame library to receive and read controller input. We also have a given header file for C++ definitions of useful functions that allow us to configure the FREE-WILi board, such as LED toggles and Display images.\\n\\n## Challenges we ran into\\n\\nThe FREE-WILi had some hardware limitations that prevented us from fully fledging out our implementation as was planned. Primarily, its I/O capabilities are limited, and cannot read controller input through USB-C as a general purpose computer can, so we had to keep the system wired to the computer during play. We also ran into space issues in uploading images to the display, so the UI is not as in depth as we would have liked.\\n\\n## Accomplishments that we\'re proud of\\n\\nFor having found out about this device only 24 hours before submission, we are very proud of the fact that we were able to flash the device with our software, turn on LEDs, upload images, and have it function properly with our controller input, despite the limitations we encountered.\\n\\n## What we learned\\n\\nWe learned a lot about how FREE-WILi works, how to write to it, and its functionalities. We also learned how to use pygame to process user inputs from a controller, to parse and control them correctly, and also how to manage multiple controllers.\\n\\n## What\'s next for Wili-Party\\n\\nIdeally, we would love to make the controller input work without running it through a computer for processing. We want the games to be able to be fully run on the FREE-WILi itself, with just two controllers as support. We would also like to make the communication wireless, so we are not cluttered with USB-C cables while playing.\\n\\n### Built With\\n\\n```c++``` ```freewili``` ```pygame``` ```python``` ```time```\\n\\n*Source* - [https://devpost.com/software/wili-party](https://devpost.com/software/wili-party)"},{"id":"wiLi-watch","metadata":{"permalink":"/blog/wiLi-watch","source":"@site/blog/2024-12-10/wili-watch.md","title":"WiLi Watch \u2013 Voice-Activated Wearable for Smart Home Control","description":"Inspiration","date":"2024-12-10T00:00:00.000Z","tags":[{"inline":false,"label":"FREE-WILi","permalink":"/blog/tags/FREE-WILi","description":"Make Embedded Systems Fun Again !"},{"inline":false,"label":"Mhacks","permalink":"/blog/tags/mhacks","description":"mhacks"},{"inline":false,"label":"Hackathon","permalink":"/blog/tags/hackathon","description":"hackathon"},{"inline":false,"label":"Student Project","permalink":"/blog/tags/studentproject","description":"studentproject"}],"readingTime":3.58,"hasTruncateMarker":true,"authors":[{"name":"Ethan McKean","url":"https://github.com/ethanmckean","imageURL":"https://lh3.googleusercontent.com/a/AEdFTp6HQQoZHtw_25wWy-SkVoz_cG9_jRL5EQWS_DZO=s96-c?height=180&width=180","key":"Ethan McKean","page":null},{"name":"Anshul Mohanty","imageURL":"https://lh3.googleusercontent.com/a/ACg8ocK9K2cEyx4J_mFaKycysLoYvU1645F9do825AwoWqDBDQi6BA=s96-c?height=180&width=180","key":"Anshul Mohanty","page":null},{"name":"Alexandra Enders","title":"Computer Engineering Student at University of Michigan","url":"https://www.linkedin.com/in/alexandra-enders-767bb21a5/","imageURL":"https://media.licdn.com/dms/image/v2/D5603AQH0frMv24KVAg/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1682174030294?e=2147483647&v=beta&t=dponlR2zkvlndYaTH8L2NwO9nBFaYsHlm3qlCnGT4Nk","key":"Alexandra Enders","page":null},{"name":"Austyn Nguyen","title":"Computer Science @ University of Michigan CoE","url":"https://www.linkedin.com/in/austyn-an-nguyen","imageURL":"https://media.licdn.com/dms/image/v2/D4E03AQHKClE_KoMA8g/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1728396078758?e=2147483647&v=beta&t=kloWr79wkHIUm5fT8rTFhoISAaz64LGOsenxFeHtvqM","key":"Austyn Nguyen","page":null}],"frontMatter":{"slug":"wiLi-watch","title":"WiLi Watch \u2013 Voice-Activated Wearable for Smart Home Control","authors":["Ethan McKean","Anshul Mohanty","Alexandra Enders","Austyn Nguyen"],"date":"2024-12-10T00:00:00.000Z","tags":["FREE-WILi","mhacks","hackathon","student project"]},"unlisted":false,"prevItem":{"title":"WiLi-Party LED Minigames","permalink":"/blog/wiLi-party"}},"content":"import YouTubeEmbed from \'@site/src/components/YouTubeEmbed\';\\n\\n<YouTubeEmbed videoId=\\"UYSAEgx3lSQ\\" />\\n\\n<br/>\\n\\n## Inspiration\\n\\nIn a world increasingly reliant on technology, the ability to interact with our surroundings effortlessly is essential, especially for individuals with mobility challenges. Enter the WiLi Watch, a wearable wristband powered by FREE-WILi and Groq aimed to empower users with independence.\\n\\n\x3c!-- truncate --\x3e\\n\\nThe journey began with a vision: to create a device that could simplify and enhance daily living for individuals with movement impairments. Our team recognized the need for an intuitive interface that would allow users to control their smart home environment without the barriers typically posed by traditional devices. By harnessing the capabilities of the watch\'s built-in infrared (IR) transmitters, we sought to bridge the gap between technology and accessibility.\\n\\n## What it does\\n\\nThe user wears an embedded wristband device that can be used to control and interact with their environment. Here is the specific tasks that can be performed wirelessly from your wrist:\\n\\n- Turn on and off lights\\n- Lock and unlock your door\\n- Display live steamed camera footage of your doorstep\\n- Bring up your GCal or the weather to be displayed These tasks can either be initiated through buttons on the wristband, or from asking out loud for certain actions to be done. We use AI to predict your needs as well based on what you say. For example, if you mention \\"Oh did I forget to lock the door?\\" the door will lock automatically, and saying \\"will I need an umbrella later?\\" will cause the weather report to go on display. Additionally, a speaker announces to you every action that is done for peace of mind. Finally, one can inquire about who is at their door and the hub will describe to you your visitor.\\n\\n## How we built it\\n\\nThis project relies on the wireless IR communication between a FREE-WILi device and a smart home \\"hub\\" consisting of an Orange Pi 5 and Arduino Nano that control other peripherals such as a camera, door \\"locking\\" system, IR receiver module, speaker, microphone, and external monitor. The frameworks we used for development are Groq for easy LLM use (Whisper for speech to text and Llama for natural language reasoning) and Cartesia for speech-text interfacing, all developed on a python virtual environment. The project is also modular to be used with Intel\'s IDC if private LLM generation is desired (we setup a flask server serving llama.cpp inferencing on a compute instance but ran out of credits)\\n\\n## Challenges we ran into\\n\\n- Using GPIO on the Orange Pi; we had to pivot after hours to integrate another microcontroller with a functional GPIO interface for motor control and IF receiving\\n- Networking compatibility; we found out later than we\'d like that the Free WiLi device which we centered our project around does not have capabilities to connect to the internet / Bluetooth so we switched to IF communication\\n- Python development on Linux; Python virtual environments are not for the weak of heart\\n\\n## Accomplishments that we\'re proud of\\n\\n- Fully functional prototype built from the ground up in 24 hours!!\\n- Door locking system on a physical prototype + doorstep camera\\n- Overcoming Free WiLi debugging hurdles in the middle of the night\\n- Kept persistence even through sleep deprivation\\n\\n## What we learned\\n\\n- Hardware hacking is challenging, but super fun!\\n- Cable management can and will stress you out\\n- Implementing APIs in a larger program\\n- Wireless communication (IR and RF)\\n- Ideation is so important\\n\\n## What\'s next for The WiLi Watch\\n\\nIncrease in functionalities available on the watch as well as real world implementation (metal instead of cardboard). A few functionalities that we came up with that could be expanded on in further development are as follows:\\n\\n- Autonomous robotic car assistant that can retrieve items\\n- Interact with kitchen appliances (turn on/off stove)\\n- Emergency response / alarm system\\n- Temperature control\\n\\n## Built With\\n\\n`arduino-nano` `cartesia` `freewili` `groq` `llama` `orange-pi` `python` `whisper`\\n\\n### Try it out\\n\\n\x3c!-- ![GitHub](./github.png) [GitHub Repo](https://github.com/ethanmckean/The-WiLi-Watch-mhacks24) --\x3e\\n\x3c!-- ![GitHub](./github.png) <a target=\\"_self\\" rel=\\"noopener noreferrer\\" href=\\"https://github.com/ethanmckean/The-WiLi-Watch-mhacks24\\" class=\\"inline-image\\">GitHub Repo</a> --\x3e\\n\\n<div class=\\"github-img\\">\\n\\n![GitHub](./github.png) <a href=\\"https://github.com/ethanmckean/The-WiLi-Watch-mhacks24\\" target=\\"_blank\\"><span>GitHub Repo</span></a>\\n</div>\\n\\n*Source* - [https://devpost.com/software/wili-watch](https://devpost.com/software/wili-watch)"}]}}')}}]);